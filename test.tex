{\rtf1\ansi\ansicpg1252\cocoartf2758
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fswiss\fcharset0 Helvetica;}
{\colortbl;\red255\green255\blue255;}
{\*\expandedcolortbl;;}
\paperw11900\paperh16840\margl1440\margr1440\vieww11520\viewh8400\viewkind0
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0\fs24 \cf0 \\documentclass[12pt,a4paper,english]\{article\}\
\\usepackage[a4paper]\{geometry\}\
\\usepackage[utf8]\{inputenc\}\
\\usepackage[OT2,T1]\{fontenc\}\
\\usepackage[keeplastbox]\{flushend\}\
\\usepackage\{color\}\
\\usepackage\{lipsum\}\
\\usepackage\{booktabs\} \
\\usepackage\{multirow\}\
\\usepackage\{tikz-cd\}\
\\usepackage\{appendix\}\
\\usepackage\{babel\}\
\\usepackage\{dsfont\}\
\\usepackage\{amsmath\}\
\\usepackage\{amssymb\}\
\\usepackage\{amsthm\}\
\\usepackage\{stmaryrd\}\
\\usepackage\{color\}\
\\usepackage\{array\}\
\\usepackage\{hyperref\}\
\\usepackage\{graphicx\}\
\\usepackage\{mathtools\}\
\\usepackage\{natbib\}\
\\setcitestyle\{authoryear,open=\{[\},close=\{]\}\} %Citation-related commands\
\\usepackage[bb=boondox]\{mathalfa\}\
\\geometry\{top=3cm,bottom=3cm,left=2.5cm,right=2.5cm\}\
\\setlength\\parindent\{0pt\}\
\\renewcommand\{\\baselinestretch\}\{1.3\}\
\
\\newcommand\\restr[2]\{\{% we make the whole thing an ordinary symbol\
  \\left.\\kern-\\nulldelimiterspace % automatically resize the bar with \\right\
  #1 % the function\
  \\vphantom\{\\big|\} % pretend it's a little taller at normal size\
  \\right|_\{#2\} % this is the delimiter\
  \}\}\
  \
% definition of the "structure"\
\\theoremstyle\{plain\}\
\\newtheorem\{thm\}\{Theorem\}[section]\
\\newtheorem\{lem\}[thm]\{Lemma\}\
\\newtheorem\{prop\}[thm]\{Proposition\}\
\\newtheorem\{coro\}[thm]\{Corollary\}\
\\newtheorem\{cla\}[thm]\{Claim\}\
\\theoremstyle\{definition\}\
\\newtheorem\{conj\}\{Conjecture\}\
\\newtheorem\{defi\}\{Definition\}\
\\newtheorem*\{ex\}\{Example\}\
\\newtheorem*\{rem\}\{Remark\}\
\\newtheorem\{step\}\{Step\}\
\
\
\\title\{Game theory\}\
\\date\{\\today\}\
\\author\{Test\\footnote\{Email:\\ \\href\{mailto:test\}\{test\}\} \\\\[0.5cm]\{\\small Advisor: Prof. test\}\}\
\
\
\\begin\{document\}\
\\maketitle\
\\newpage\
\
\\tableofcontents\
\\newpage\
\
\\begin\{abstract\}\
The definition in the thesis is recollected from the book \\citep*\{karlin2017game\}.\
\
\\end\{abstract\}\
\\newpage\
\
\\section\{Introduction\}\
\
\
\\newpage\
\\section\{Classical game theory\}\
\\subsection\{Strategic-Form Games\}\
In game theory, the strategic form (or normal form) is a way of describing a game using a matrix \\ref\{tab:strategic form\}. The game is defined by exhibiting on each side of the matrix the different players (here players 1 and 2), each strategy or choice they can make (here strategies $A$ and $B$), and sets of payoffs they will each receive for a given strategy $(p_\{1A\}, p_\{2A\}; p_\{1A\}, p_\{2B\}; p_\{1B\}, p_\{2A\}; p_\{1B\}, p_\{2B\})$.\
\
\\begin\{table\}[!ht]\
    \\centering\
    \\begin\{tabular\}\{cc|c|c|\}\
         &  & Player2 & \\\\\
         &  & Strategy $A$ & Strategy $B$\\\\\
         \\midrule %[2pt]\
        player1 & Strategy $A$ & $P_\{1A\},P_\{2A\}$ & $P_\{1A\}, P_\{2B\}$\\\\\
         & Strategy $B$ & $P_\{1B\},P_\{2A\}$ & $P_\{1B\},P_\{2B\}$\\\\\
    \\end\{tabular\}\
    \\caption\{Example of Strategic Form\}\
    \\label\{tab:strategic form\}\
\\end\{table\}\
\\subsection\{Extensive-Form Games\}\
\
The strategic form allows us to efficiently analyze all potential outcomes of a game. In the provided matrix, if player 1 chooses strategy $A$ and player 2 chooses strategy $B$, the resulting payoff set would be denoted as $p_\{1A\}$, $p_\{2B\}$. Similarly, if player 1 chooses strategy $B$ and player 2 chooses strategy $A$, the payoff set would be $p_\{1B\}$, $p_\{2A\}$. The strategic form is commonly used to describe simultaneous games, where both players make their choices simultaneously. In contrast, sequential games are better represented using the extensive form (or tree form). It is worth noting that simultaneous games assume the presence of complete and imperfect information, and the game rules as well as each player's payoffs are considered common knowledge.\
\\section\{Nash Equilibrium\}\
\
Nash equilibrium is a concept in game theory that was introduced by John Nash in 1950  where the optimal outcome of a game is one where no player has an incentive to deviate from their chosen strategy after considering an opponent's choice \\citep*\{nash1950equilibrium,nash1951non\}. In other words, each player's strategy is optimal given what the other players are doing.\
\
This equilibrium concept is most used in economics, computing, evolutionary biology, artificial intelligence, military theory, and political science. It can apply to games with any number of players and allows for probabilistic (mixed) strategies.\
\
The Nash Equilibrium provides a prediction of what will happen if several people or several institutions are making decisions at the same time, and the outcome depends on the decisions of the others. If the game is in the Nash Equilibrium, players choose their best strategy given the strategies that all the other players have chosen, regardless of the benefits to the group.\
\\subsection\{Examples for games with Nash equilibria\}\
For a more formal infusion of Nash's theory, it is imperative to first delineate the concept of the prisoner's dilemma prior to discussing general-sum games since Nash equilibria can be found in various types of games, including zero-sum games, general-sum games, and games with perfect or imperfect information.\
\\subsubsection\{Zero Sum Game\}\
Let's begin with an example:\
\\begin\{ex\}[Rock-Paper-Scissors]\\label\{ex:two-person-zero-sum\}\
    Consider a typical game of Rock-Paper-Scissors involving two players, where each player simultaneously selects one of three available options: Rock, Paper, or Scissors without other complex psychological factors. The game is defined as a zero-sum because the payoff for one player is essentially the loss for the other. To illustrate, should player 1 emerge victorious, they receive a payoff of +1, which coincides with a payoff of -1 for player 2.\
\
The Nash equilibrium is achieved within this game when each player selects their choice with an equal probability. This is understood as each player choosing Rock, Paper, or Scissors with a 1/3 probability, ensuring that no player can enhance their own payoff by unilaterally altering their strategy. The foundation of this logic is rooted in the fact that if a player chooses a different option with higher probability, their opponent can consequently exploit this by more frequently selecting the countering option, eventually leading to a decreased payoff for the initial player.\
\
The payoff matrix is introduced below in \\ref\{tab:paperock\},\
\\begin\{table\}[!ht]\
\\label\{tab:paperock\}\
\\centering\
\\begin\{tabular\}\{|l|llll|\}\
\\hline\
                  & \\multicolumn\{4\}\{l|\}\{Player 1\}                                                    \\\\ \\hline\
\\multirow\{4\}\{*\}\{Player 2\} &  & \\multicolumn\{1\}\{l|\}\{Paper\} & \\multicolumn\{1\}\{l|\}\{Rock\} & \\multicolumn\{1\}\{l|\}\{Scissor\} \\\\ \\cline\{2-5\} \
                  & \\multicolumn\{1\}\{l|\}\{Paper\} & \\multicolumn\{1\}\{l|\}\{(0,0)\} & \\multicolumn\{1\}\{l|\}\{(0,1)\} & (1,0) \\\\ \\cline\{2-5\} \
                  & \\multicolumn\{1\}\{l|\}\{Rock\} & \\multicolumn\{1\}\{l|\}\{(1,0)\} & \\multicolumn\{1\}\{l|\}\{(0,0)\} & (0,1)  \\\\ \\cline\{2-5\} \
                  & \\multicolumn\{1\}\{l|\}\{Scissor\} & \\multicolumn\{1\}\{l|\}\{(0,1)\} & \\multicolumn\{1\}\{l|\}\{(1,0)\} &  (0,0)\\\\ \\hline\
\\end\{tabular\}\
\\caption\{The payoff matrix of Game: Rock-Paper-Scissors\}\
\\end\{table\}\
in this matrix, each item (such as (0,1)) indicates that Player 1 wins and Player 2 loses.\
\
Hence, the Nash equilibrium in this game is realized when both players select their move with equal probability, culminating in an expected payoff of zero for each player.\
\\end\{ex\}\
\
With the intuition from the example \\ref\{ex:two-person-zero-sum\} above, the definition of payoff matrix of two person zero-sum game is as following: \
\\begin\{defi\}[Payoff Matrix of Zero-Sum game]\
    In a two-person zero-sum game, we can represent the game by a payoff matrix $A=(a_\{i,j\})_\{m\\times n\}$. Here, the rows indexed by $i \\in \\\{1, ..., m\\\}$ represent all the possible strategies for player $1$, and the columns indexed by $j \\in \\\{1, ..., n\\\}$ correspond to all the possible strategies for player $2$. The entry $a_\{i,j\}$ in the matrix $A$ denotes the payoff when player $1$ chooses strategy $i$ and player $2$ chooses strategy $j$, with each player unknowing of the other's choice.\
\\end\{defi\}\
\
As previously discussed in example \\ref\{ex:two-person-zero-sum\}, it's possible to implement strategies wherein each action is selected with a certain probability, which we refer to as a mixed strategy.\
\
\\begin\{defi\}[Mixed Strategy of Zero-Sum Game]\\label\{def:mixed\}\
    A strategy in which each action is selected with some probability is a mixed strategy.\
\
    A mixed strategy for player 1 is determined by a vector $\\mathbf\{x\}=(x_\{1\},...,x_\{m\})^\{T\}$ where $x_\{i\}$ denotes the probability to choose strategy $i$. The set of mixed strategies for player 1 is denoted by \
    \\begin\{equation\}\
        \\Delta_\{m\}=\\bigg\\\{x_\{i\}|x_\{i\}\\in\\mathbb\{R\}_\{\\geq 0\}, \\text\{and\}\\ \\sum^\{m\}_\{i=1\}x_\{i\}=1\\bigg\\\}.\
    \\end\{equation\}\
\
    Similarly, the set of mixed strategies for player 2 is denoted by \
    \\begin\{equation\}\
        \\Delta_\{n\}=\\bigg\\\{y_\{j\}|y_\{i\}\\in\\mathbb\{R\}_\{\\geq 0\}, \\text\{and\}\\ \\sum^\{n\}_\{j=1\}y_\{j\}=1\\bigg\\\},\
    \\end\{equation\}\
\\end\{defi\}\
in which the notation $\\mathbb\{R\}_\{\\geq 0\}$ is defined as $\\\{x\\in\\mathbb\{R\}|x\\geq0\\\}$.\
If player 1 opts to implement strategy $\\mathbf\{x\}$ and player 2 adopts strategy $\\mathbf\{y\}$, the expected gain for player 1 (which equates to the expected loss for player 2) is given by:\
\
\\begin\{equation\}\
\\mathbf\{x\}^\{T\}A\\mathbf\{y\}=\\sum_\{i=1\}^\{m\}\\sum^\{n\}_\{j=1\}x_\{i\}a_\{ij\}y_\{j\}.\
\\end\{equation\}\
\
\
One of the seminal results in game theory is the Minimax Theorem, which was demonstrated by mathematician John von Neumann \\citep*\{v1928theorie\}. This theorem important in zero-sum two-player games, where the loss of one player equates to the gain of the other player. Von Neumann's theorem proposes optimal strategies for both players that yield the greatest benefit for each, given that the other player is also acting optimally.\
\
The methods used in von Neumann \\& Morgenstern is perhaps best-known \\citep*\{von1947theory\}, applying the separating hyperplane theorem. This approach, along with von Neumann\'92s original proof, used properties unique to the real numbers. This includes the Maximum Theorem for continuous functions on compact sets. However, it's important to note that the Minimax Theorem is not limited to real numbers.\
\
As Kuhn \\& Tucker demonstrated, the Minimax Theorem holds true when payoffs are found in any ordered field $F$ \\citep*\{kuhn1953contributions\}. Mixed strategies are allowed to range over F. Their proof depended on a result from Stiemke \\citep*\{stiemke1915positive\}, a predecessor of duality results in linear programming. Indeed, linear-programming duality (or related results such as Farkas\'92 Lemma) offers a swift proof of the Minimax Theorem, as exemplified in Vohra \\citep*\{vohra2004advanced\}.\
For further historical context and developments related to von Neumann's minimax theorem, additional information can be found in the study by \\citep*\{kjeldsen2001john\}. \
\
\
\\begin\{thm\}[Von Neumann's Minimax Theorem]\\label\{thm:vonneumann\}\
For any finite two-person zero-sum game $G$,\
    Let $A$ be an $m\\times n$ payoff matrix, and let $\\Delta_\{m\}$ and $\\Delta_\{n\}$ be as defined in \\ref\{def:mixed\}. Then the following equality holds true: \
\\begin\{equation\}\
\\text\{max\}_\{\\mathbf\{x\}\\in\\Delta_\{m\}\}\\text\{min\}_\{\\mathbf\{y\}\\in\\Delta_\{n\}\}\\mathbf\{x\}^\{T\}A\\mathbf\{y\}=V=\\text\{min\}_\{\\mathbf\{y\}\\in\\Delta_\{n\}\}\\text\{max\}_\{\\mathbf\{x\}\\in\\Delta_\{m\}\}\\mathbf\{x\}^\{T\}A\\mathbf\{y\}.\
\\end\{equation\} \
This quantity $V$ is referred to as the value of the two-person zero-sum game characterized by the payoff matrix $A$.\
\\end\{thm\}\
\
Moving forward, we will utilize the proof by duality of linear programming. This offers a more succinct approach compared to the convex geometry method articulated in \\citep*\{karlin2017game\}.\
\
\\begin\{proof\}\
\
Note that for a selected strategy $x$, the payoff $\\min _\{y \\in \\Delta_n\} y A x$ can be viewed as a simplistic linear programming problem, subjected to the constraint $\\left\\\{y \\geq 0, \\sum_j y_j=1\\right\\\}$. The vertices of the corresponding polytope are defined by the vectors $\\left\\\{e_i\\right\\\}_\{i=1\}^n$. Each $e_i$ is a vector with 1 at the $i$-th position and zero elsewhere, rendering it an 'indicator' vector, highlighting the selected strategy in the $i$-th position.\
\
In essence, we can always presuppose that the second player will consistently choose a pure strategy to secure the optimal payoff for herself, as shown by the equations $$\
\\max _\{x \\in \\Delta_m\} \\min _\{y \\in \\Delta_n\} y A x=\\max _\{x \\in \\Delta_m\} \\min _i(A x)_i\
$$ and $$\
\\min _\{y \\in \\Delta_n\} \\max _\{x \\in \\Delta_m\} y A x=\\min _\{y \\in \\Delta_n\} \\max _j(y A)_j.\
$$ \
\
The inequalities in represent different strategic responses:\
$\\max_\{x \\in \\Delta_m\} \\min_\{y \\in \\Delta_n\} yAx$ represents that player 1 plays first and player 2 uses her optimal response.\
$\\min_\{y \\in \\Delta_n\} \\max_\{x \\in \\Delta_m\} yAx$ represents that player 2 plays first and player 1 uses his optimal response.\
Therefore, to prove the theorem, we only need to establish the equality \
\\begin\{equation\}\\label\{equality1\}\
    \\max _\{x \\in \\Delta_m\} \\min _i(A x)_i=\\min _\{y \\in \\Delta_n\} \\max _j(y A)_j\
\\end\{equation\}\
\
We will now formulate the Left-Hand Side (LHS) of equation \\ref\{equality1\} as a linear programming problem as follows:\
  \\begin\{equation\}\
      \\begin\{aligned\}\
\\text \{ (Primal) : \} & \\max t \\\\\
\\text \{ s.t. \} & \\sum_j a_\{i j\} x_j \\geq t \\\\\
& \\sum_j x_j=1 \\\\\
& x \\geq 0 \\\\\
& t \\not= 0\
\\end\{aligned\}\
  \\end\{equation\}\
\
The dual problem corresponding to the linear programming problem can then be articulated as follows:\
  \\begin\{equation\}\
      \\begin\{aligned\}\
\\text \{ (Dual) : \} & \\min \\quad w \\\\\
\\text \{ s.t. \} & y \\geq 0 \\\\\
& w \\gtrless 0 \\\\\
& w-\\sum_i y_i a_\{i j\} \\geq 0 \\\\\
& \\sum_i y_i=1\
\\end\{aligned\}\
  \\end\{equation\}\
It can easily be inferred that the primal problem is feasible (choosing any $x \\in \\Delta_m$ and $t=\\min_\{ij\}a_\{ij\}$ ensures feasibility) and bounded (by $\\max_\{ij\}a_\{ij\}$). Therefore, according to the duality theorem, both the primal and dual problems have optimal solutions and, at optimality, share identical optimal values. We further reformulate the dual problem as\
  \\begin\{equation\}\
      \\begin\{aligned\}\
\\text \{ (Dual) : \} & \\min w \\\\\
\\text \{ s.t. \} & \\sum_i y_i a_\{i j\} \\leq w \\\\\
& y \\in \\Delta_m \\\\\
& w \\not= 0\
\\end\{aligned\}\
  \\end\{equation\}\
\
  Which simplifies to $\\min _\{y \\in \\Delta_m\} \\max _j \\sum_i y_i a_\{i j\}$, corresponding precisely to the Right-Hand Side (RHS) of equation \\ref\{equality1\}. Therefore, with the equality established, we have successfully provided a proof of the minimax theorem.\
\
\
Suppose $x^*, y^*$ are optimal solutions for the RHS and LHS of equation \\ref\{equality1\} respectively, such that $y^* A x^*$ denotes the common value of equation \\ref\{equality1\}. From the preceding proof, we deduce that $t^*=w^*=y^* A x^*$. Now, applying the complementary slackness principle to this optimal solution, we can ascertain the corresponding values for each $i$ and $j$ as follows:\
\\begin\{equation\}\
    \\begin\{gathered\}\
\\sum_j a_\{i j\} x_j^*>t^* \\rightarrow y_i^*=0 \\\\\
\\sum_i y_i^* a_\{i j\}<w^* \\rightarrow x_j^*=0\
\\end\{gathered\}\
\\end\{equation\}\
If $x^*$ is deemed optimal, and $I$ designates the set of indices for the inequalities $t^*-\\sum_j a_\{i j\} x_j^* \\leq 0$ that are stringent, then it holds that $y_i^*=0$ for all $i \\in I$. This is equivalent to stating the player $y$ cannot assign any positive probability to a strategy that does not optimize against $x^*$. The non-optimality of the $i^\{th\}$ strategy implies $y_i^*=0$. This reasoning is symmetrically applicable for the $x$ player. Conversely, if $x, y$ satisfy these relations, they are deemed to be optimal.\
\\end\{proof\}\
\\begin\{rem\}\
    Within the proof of the theorem above, it delivers an invaluable mathematical insight. If Player 1 selects a strategy prior to Player 2, then Player 1 can ensure a payoff of at least $\\text\{max\}_\{\\mathbf\{x\}\\in\\Delta_\{m\}\}\\text\{min\}_\{\\mathbf\{y\}\\in\\Delta_\{n\}\}\\mathbf\{x\}^\{T\}A\\mathbf\{y\}$.\
\
Conversely, if Player 2 defines a strategy before Player 1, then Player 2 can guarantee that Player 1 receives a maximum of $\\text\{min\}_\{\\mathbf\{y\}\\in\\Delta_\{n\}\}\\text\{max\}_\{\\mathbf\{x\}\\in\\Delta_\{m\}\}\\mathbf\{x\}^\{T\}A\\mathbf\{y\}$.\
\
This consequently entails that \
\
\\begin\{equation\}\
\\text\{max\}_\{\\mathbf\{x\}\\in\\Delta_\{m\}\}\\text\{min\}_\{\\mathbf\{y\}\\in\\Delta_\{n\}\}\\mathbf\{x\}^\{T\}A\\mathbf\{y\}\\leq\\text\{min\}_\{\\mathbf\{y\}\\in\\Delta_\{n\}\}\\text\{max\}_\{\\mathbf\{x\}\\in\\Delta_\{m\}\}\\mathbf\{x\}^\{T\}A\\mathbf\{y\}.\
\\end\{equation\} \
\\end\{rem\}\
\\begin\{rem\}\
    In the proof, we used the theorem about complementary slackness principle, In the context of optimization, particularly linear programming, the Complementary Slackness Principle is a key result that provides conditions for optimality. It essentially connects the primal and dual versions of a linear programming problem.\
Assume we are dealing with a primal problem of the type:\
Maximize $c^T x$ subject to $Ax \\leq b$ and $x \\geq 0$.\
And its corresponding dual problem:\
Minimize $b^T y$ subject to $A^T y \\geq c$ and $y \\geq 0$.\
Then, the Complementary Slackness Principle states:\
Given primal feasible $x$ and dual feasible $y$, $x$ and $y$ are optimal for their respective problems if and only if for each i=1,...,n, we have:\
1) $x_i > 0$ implies $(A^T y - c)_i = 0$ (i.e., constraints of the dual are binding at the optimum if the corresponding primal variable is positive).\
and\
2) $y_i > 0$ implies $(Ax - b)_i = 0$ (i.e., constraints of the primal are binding at the optimum if the corresponding dual variable is positive).\
Here, $x$ are primal variables and $y$ are dual variables. The transposed matrix is denoted as $A^T$.\
In simple terms, complementary slackness means that, at optimality, if one of a pair of primal/dual variables is non-zero, the other one must be zero. They are called "complementary" because they always multiply to zero.\
This result is instrumental in simplifying the solution of optimization problems and checking optimality conditions \\citep*\{bazaraa2011linear\}.\
\\end\{rem\}\
We will now present a proof of the von Neumann Minimax Theorem proceeding from an alternative perspective, which notably diverges from the typical approach that employs linear programming. Our method of proof will instead lean heavily on a fundamental theorem from convex geometry \\citep*\{karlin2017game\}.\
Following this, a distinctive, constructive proof will be presented correspondingly in Section 18.4.3. This approach will provide a different set of insights into the theorem's establishment and application.\
\
Firstly, let us reintroduce the concept of a (Euclidean) norm. In relation to a vector, say $\\mathbf\{v\}$, its (Euclidean) norm denotes the (Euclidean) distance from $\\mathbf\{0\}$ (the origin) to the point represented by $\\mathbf\{v\}$. It is denoted by $\\|\\mathbf\{v\}\\|$. Hence, the norm of the vector $\\mathbf\{v\}$ is defined as $\\|\\mathbf\{v\}\\| = \\sqrt\{\\mathbf\{v\}^T \\mathbf\{v\}\}$.\
Shifting our attention to metric spaces, a subset of such a space is termed "closed" if it encompasses all of its limit points. In contrast, the term "bounded" is used to characterize a subset that can be entirely enclosed within a ball having a certain finite radius, say $R$. These terms lay the base for our ensuing discussions pertaining to convex geometry.\
\
\\begin\{defi\}\
    A subset $K \\subseteq \\mathbb\{R\}^d$ earns the descriptor of being "convex" if, given any two distinct points $\\mathbf\{a\}, \\mathbf\{b\}$ that belong to $K$, the entire line segment that spans between them fully resides within $K$ as well. To put this into mathematical terms, for every pair of points $\\mathbf\{a\}, \\mathbf\{b\} \\in K$ and for every real number $p$ that lies in the interval $[0,1]$,\
$$\
p \\mathbf\{a\}+(1-p) \\mathbf\{b\} \\in K ,\
$$\
holds, thereby encapsulating the notion of convexity within $K$.\
\\end\{defi\}\
\\begin\{thm\}[The Separating Hyperplane Theorem]\\label\{thm:sephyper\}\
    for a given set $K \\subseteq \\mathbb\{R\}^d$ that is closed and convex, if the origin $\\mathbf\{0\}$ does not belong within $K$, then there exist a vector $\\mathbf\{z\} \\in \\mathbb\{R\}^d$ and a scalar $c \\in \\mathbb\{R\}$ that satisfy the following inequality:\
$$\
0 < c < \\mathbf\{z\}^T \\mathbf\{v\}\
$$\
for all vectors $\\mathbf\{v\} \\in K$.\
\\end\{thm\}\
\\begin\{rem\}\
    This theorem provides insight into the geometrical properties of a convex set $K$ and assures the existence of a hyperplane that separates the origin from the set, when the origin is not a member of the set.\
\
The vector denoted by $\\mathbf\{0\}$ represents a vector composed entirely of zeros. The essence of the Separating Hyperplane Theorem is that it guarantees the existence of a hyperplane, which can be visualized as a line in two dimensions, a plane in three dimensions, or more generally, an affine $\\mathbb\{R\}^\{d-1\}$-subspace in $\\mathbb\{R\}^d$, that segregates $\\mathbf\{0\}$ from the set $K$.\
This implies that there is a particular point on the hyperplane that any continuous path from $\\mathbf\{0\}$ to the set $K$ must cross. The equation of this separating hyperplane is represented by $\\left\\\{\\mathbf\{x\} \\in \\mathbb\{R\}^d: \\mathbf\{z\}^T \\mathbf\{x\}=c\\right\\\}$.\
Additionally, the theorem designates the origin $\\mathbf\{0\}$ to lie in the half-space $\\\{\\mathbf\{x\} \\in \\mathbb\{R\}^d: \\mathbf\{z\}^T \\mathbf\{x\}$. Conversely, the convex set $K$ is found within the opposite half-space, characterized by $\\left\\\{\\mathbf\{x\} \\in \\mathbb\{R\}^d: \\mathbf\{z\}^T \\mathbf\{x\}>c\\right\\\}$. This constitutes a clear distinction between the positions of the origin and the convex set within the space.\
\
As depicted in Figure \\ref\{fig:convex\}, Theorem \\ref\{thm:sephyper\} can be interpreted geometrically in a meaningful way.\
\\begin\{figure\}\
    \\centering\
    \\includegraphics[width=0.7\\linewidth]\{convex.png\}\
    \\caption\{Hyperplane separating the closed convex body $K$ from $\\mathbf\{0\}$.\}\
    \\label\{fig:convex\}\
\\end\{figure\}\
\\end\{rem\}\
In what follows, the metric is the Euclidean metric.\
\
\\begin\{proof\}[Proof of theorem \\ref\{thm:sephyper\}]\
    Let's select the radius $r$ so that the closed ball $B_r = \\\{\\mathbf\{x\} \\in \\mathbb\{R\}^d:\\|\\mathbf\{x\}\\| \\leq r\\\}$ intersects the set $K$. Consequently, the function $\\mathbf\{w\} \\mapsto \\|\\mathbf\{w\}\\|$ mapping from $K \\cap B_r$ to $[0, \\infty)$ becomes continuous, with a domain that is nonempty, bounded, and closed (as depicted in Figure \\ref\{fig:sephyper\}).\
With these properties, the function is guaranteed to attain its minimum value, known as the infimum, at some specific point $\\mathbf\{z\}$ within the set $K$. For this particular point $\\mathbf\{z\} \\in K$, we thereby conclude:\
$$ \\|\\mathbf\{z\}\\| = \\inf_\{\\mathbf\{w\} \\in K\} \\|\\mathbf\{w\}\\| . $$\
This equation signifies that the norm of $\\mathbf\{z\}$ equals the smallest norm among all vectors in $K$.\
\
\\begin\{figure\}\
    \\centering\
    \\includegraphics[width=0.7\\linewidth]\{sephyper.png\}\
    \\caption\{Intersecting $K$ with a ball to get a nonempty closed bounded domain.\}\
    \\label\{fig:sephyper\}\
\\end\{figure\}\
\
\
Suppose $\\mathbf\{v\}$ belongs to set $K$. Taking advantage of the convex property of set $K$, it is evident that for any given $\\varepsilon$ in the range $(0, 1)$, the following relationship holds:\
$$ \\varepsilon \\mathbf\{v\}+(1-\\varepsilon) \\mathbf\{z\} = \\mathbf\{z\}-\\varepsilon\\left(\\mathbf\{z\}-\\mathbf\{v\}\\right) \\in K. $$\
The leftmost term describes a weighted point within $K$ between $\\mathbf\{v\}$ and $\\mathbf\{z\}$. On the rightmost, it can be reinterpreted as a displacement of the point $\\mathbf\{z\}$ by a vector $\\varepsilon\\left(\\mathbf\{z\}-\\mathbf\{v\}\\right)$.\
Considering that $\\mathbf\{z\}$ possesses the lowest norm among all points in $K$, it follows that:\
$$ \\|\\mathbf\{z\}\\|^2 \\leq \\|\\mathbf\{z\} - \\varepsilon(\\mathbf\{z\}-\\mathbf\{v\})\\|^2 = \\|\\mathbf\{z\}\\|^2 - 2\\varepsilon \\mathbf\{z\}^T(\\mathbf\{z\}-\\mathbf\{v\}) + \\varepsilon^2\\|\\mathbf\{z\}-\\mathbf\{v\}\\|^2. $$\
Reorganizing the terms leads to:\
$$ 2\\varepsilon \\mathbf\{z\}^T(\\mathbf\{z\}-\\mathbf\{v\}) \\leq \\varepsilon^2 \\|\\mathbf\{z\}-\\mathbf\{v\}\\|^2, $$\
which can be further simplified to:\
$$ \\mathbf\{z\}^T(\\mathbf\{z\}-\\mathbf\{v\}) \\leq \\frac\{\\varepsilon\}\{2\} \\|\\mathbf\{z\}-\\mathbf\{v\}\\|^2. $$\
This inequality establishes a key relation in the context of vectors within our particular convex set $K$.\
\
As we allow $\\varepsilon$ to approach 0, the inequality simplifies to:\
$$ \\mathbf\{z\}^T(\\mathbf\{z\}-\\mathbf\{v\}) \\leq 0, $$\
which implicitly leads us to:\
$$ \\|\\mathbf\{z\}\\|^2 \\leq \\mathbf\{z\}^T \\mathbf\{v\}. $$\
Given that $\\mathbf\{z\}$ belongs to $K$ and $\\mathbf\{0\}$ does not belong to $K$, it certainly follows that $\\|\\mathbf\{z\}\\| > 0$. If we set $c = \\frac\{1\}\{2\}\\|\\mathbf\{z\}\\|^2$, we find that $c$ lies strictly between $0$ and $\\mathbf\{z\}^T \\mathbf\{v\}$ for all $\\mathbf\{v\} \\in K$, i.e.,\
$$ 0 < c < \\mathbf\{z\}^T \\mathbf\{v\}, $$\
which neatly brings us to the desired conclusion of our argument.\
\\end\{proof\}\
we need a lemma to prove the theorem \\ref\{thm:vonneumann\}, which serves as a mathematical formulation of the concept of the minimax inequality in game theory, providing fundamental insight on the interaction between optimization and decision-making in a two-player setting.\
\
\\begin\{lem\}\\label\{lem:minmax\}\
    Consider two sets $X$ and $Y$ in $\\mathbb\{R\}^d$, which are both closed and bounded. Let $f: X \\times Y \\rightarrow \\mathbb\{R\}$ be a continuous function mapping from the product space $X \\times Y$ to the real numbers.\
Then, for this function $f$, the following inequality holds:\
\\begin\{equation\}\\label\{2.13\}\
    \\max _\{\\mathbf\{x\} \\in X\} \\min _\{\\mathbf\{y\} \\in Y\} f(\\mathbf\{x\}, \\mathbf\{y\}) \\leq \\min _\{\\mathbf\{y\} \\in Y\} \\max _\{\\mathbf\{x\} \\in X\} f(\\mathbf\{x\}, \\mathbf\{y\}).\
\\end\{equation\}\
\\end\{lem\}\
\\begin\{proof\}[Proof of lemma \\ref\{lem:minmax\}]\
    First, we prove a foundational lemma using the particular case where $X$ and $Y$ are finite sets. We don't make any specific assumptions about the function $f$.\
Consider an arbitrary pair $(\\tilde\{\\mathbf\{x\}\}, \\tilde\{\\mathbf\{y\}\})$ from the cartesian product $X \\times Y$. We can observe that for a given $\\tilde\{\\mathbf\{x\}\}$, \\begin\{equation\}\
\\min _\{\\mathbf\{y\} \\in Y\} f(\\tilde\{\\mathbf\{x\}\}, \\mathbf\{y\}) \\leq f(\\tilde\{\\mathbf\{x\}\}, \\tilde\{\\mathbf\{y\}\}) \\leq \\max _\{\\mathbf\{x\} \\in X\} f(\\mathbf\{x\}, \\tilde\{\\mathbf\{y\}\}) .\
\\end\{equation\}\
The above inequality holds for every $\\tilde\{\\mathbf\{x\}\}$ in $X$, hence, we can infer that \\begin\{equation\}\
\\max _\{\\tilde\{\\mathbf\{x\}\} \\in X\} \\min _\{\\mathbf\{y\} \\in Y\} f(\\tilde\{\\mathbf\{x\}\}, \\mathbf\{y\}) \\leq \\max _\{\\mathbf\{x\} \\in X\} f(\\mathbf\{x\}, \\tilde\{\\mathbf\{y\}\}) .\
\\end\{equation\}\
Finally, if we minimize over $\\tilde\{\\mathbf\{y\}\}$ in $Y$, we obtain the target inequality mentioned in Equation \\ref\{2.13\}.\
\
In order to generalize this lemma to arbitrary sets of possible $X$ and $Y$, we need to ensure the maxima and minima in question indeed exist. Because continuous functions attain their minima on compact sets, we can see that the function $g(\\mathbf\{x\})=\\min _\{\\mathbf\{y\} \\in Y\} f(\\mathbf\{x\}, \\mathbf\{y\})$ is well-defined.\
Continuing, we note the continuity of $f$ and the compactness of the product set $X \\times Y$, which necessitates that $f$ is uniformly continuous on $X \\times Y$. We can formally denote this as:\
\\begin\{equation\}\
\\forall \\epsilon > 0, \\exists \\delta > 0, \\textrm\{ such that if \} ||\\mathbf\{x\}_1-\\mathbf\{x\}_2|| < \\delta, \\textrm\{ then \} ||f(\\mathbf\{x\}_1, \\mathbf\{y\})-f(\\mathbf\{x\}_2, \\mathbf\{y\})|| \\leq \\epsilon.\
\\end\{equation\}\
From which it follows that $||g(\\mathbf\{x\}_1)-g(\\mathbf\{x\}_2)|| \\leq \\epsilon$. Hence, we see that $g: X \\rightarrow \\mathbb\{R\}$ is a continuous function. Therefore, $\\max_\{\\mathbf\{x\} \\in X\} g(\\mathbf\{x\})$ is guaranteed to exist.\
\\end\{proof\}\
\
Now we are ready to prove the Von Neumann\'92s minimax theorem:\
\
\\begin\{thm\}[Von Neumann's Minimax Theorem \\citep*\{karlin2017game\}]\\label\{thm:vonNeumannMinimax\}\
Let $A$ be an $m \\times n$ payoff matrix, and let $\\Delta_m=\\left\\\{\\mathbf\{x\} \\in \\mathbb\{R\}^m: \\mathbf\{x\} \\geq \\mathbf\{0\}, \\sum_i x_i=1\\right\\\}$ and $\\Delta_n=\\left\\\{\\mathbf\{y\} \\in \\mathbb\{R\}^n\\right.$ : $\\left.\\mathbf\{y\} \\geq \\mathbf\{0\}, \\sum_j y_j=1\\right\\\}$. Then\
\\begin\{equation\}\\label\{2.14\}\
    \\max _\{\\mathbf\{x\} \\in \\Delta_m\} \\min _\{\\mathbf\{y\} \\in \\Delta_n\} \\mathbf\{x\}^T A \\mathbf\{y\}=\\min _\{\\mathbf\{y\} \\in \\Delta_n\} \\max _\{\\mathbf\{x\} \\in \\Delta_m\} \\mathbf\{x\}^T A \\mathbf\{y\} .\
\\end\{equation\}\
\
As we discussed earlier, this quantity is called the value of the two-person zero-sum game with payoff matrix $A$.\
\\end\{thm\}\
\\begin\{proof\}[Proof of theorem \\ref\{thm:vonNeumannMinimax\}]\
We can deduce the following inequality: \\begin\{equation\}\
\\max_\{\\mathbf\{x\} \\in \\Delta_m\} \\min_\{\\mathbf\{y\} \\in \\Delta_n\} \\mathbf\{x\}^T A \\mathbf\{y\} \\leq \\min_\{\\mathbf\{y\} \\in \\Delta_n\} \\max_\{\\mathbf\{x\} \\in \\Delta_m\} \\mathbf\{x\}^T A \\mathbf\{y\}\
\\end\{equation\}\
straightforwardly from the previously discussed lemma (Lemma \\ref\{lem:minmax\}), since we're considering $f(\\mathbf\{x\}, \\mathbf\{y\}) = \\mathbf\{x\}^T A \\mathbf\{y\}$ as a function that is continuous in both variables. Furthermore, both $\\Delta_m$ and $\\Delta_n$, which are subsets of $\\mathbb\{R\}^m$ and $\\mathbb\{R\}^n$ respectively, are closed and bounded. \
\
We aim to demonstrate that the left-hand side of inequality \\ref\{2.14\} is at least as great as the right-hand side. To illustrate this, we assume that:\
\\begin\{equation\}\\label\{2.15\}\
\\lambda<\\min_\{\\mathbf\{y\} \\in \\Delta_n\} \\max_\{\\mathbf\{x\} \\in \\Delta_m\} \\mathbf\{x\}^T A \\mathbf\{y\}\
\\end\{equation\}\
We then introduce a new game with a revised payoff matrix denoted by $\\hat\{A\}$. Here, each element is defined as $\\hat\{a\}_\{i, j\}=a_\{i j\}-\\lambda$. For this updated game, the following strict inequality holds:\
\\begin\{equation\}\\label\{2.16\}\
0<\\min_\{\\mathbf\{y\} \\in \\Delta_n\} \\max_\{\\mathbf\{x\} \\in \\Delta_m\} \\mathbf\{x\}^T \\hat\{A\} \\mathbf\{y\}\
\\end\{equation\}\
\
Consider every mixed strategy $\\mathbf\{y\}$ in the set $\\Delta_n$ for player II. This gives a corresponding gain vector $\\hat\{A\} \\mathbf\{y\}$ in $\\mathbb\{R\}^m$.\
Next, let's define a set $K$, which will embody all vectors that dominate some gain vector $\\hat\{A\}\\mathbf\{y\}$. More formally, we have:\
\\begin\{equation\}\
K = \\\{\\hat\{A\} \\mathbf\{y\}+\\mathbf\{v\}: \\mathbf\{y\} \\in \\Delta_n, \\mathbf\{v\} \\in \\mathbb\{R\}^m, \\mathbf\{v\} \\geq \\mathbf\{0\}\\\}\
\\end\{equation\}\
\
The set $K$ embodies convexity and is closed. This can be explained as follows: given that $\\Delta_n$ is closed, bounded, and convex, and considering that the set $\\\{\\mathbf\{v\} \\in \\mathbb\{R\}^m | \\mathbf\{v\} \\geq \\mathbf\{0\}\\\}$ is also closed and convex.\
Furthermore, it is impossible for $K$ to include the zero vector $\\mathbf\{0\}$. To understand why, let's assume, for the sake of contradiction, that $\\mathbf\{0\}$ belongs to $K$. It would mean there exists a mixed strategy $\\mathbf\{y\}$ in $\\Delta_n$ satisfying $\\hat\{A\} \\mathbf\{y\} \\leq \\mathbf\{0\}$.\
This, however, leads to a contradiction since it would suggest that $\\max_\{\\mathbf\{x\} \\in \\Delta_m\} \\mathbf\{x\}^T \\hat\{A\} \\mathbf\{y\} \\leq 0$, which contradicts the inequality from equation \\ref\{2.16\}.\
Therefore, it must be concluded that $\\mathbf\{0\}$ does not belong to $K$.\
\
The set $K$ indeed satisfies the conditions of the Separating Hyperplane Theorem (Theorem \\ref\{thm:sephyper\}). Thus, by applying this theorem, we can find $\\mathbf\{z\} \\in \\mathbb\{R\}^m$ and some positive real number $c$ so that the inequality $\\mathbf\{z\}^T \\mathbf\{w\}>c>0$ is satisfied for every $\\mathbf\{w\} \\in K$.\
This can be expressed as:\
\\begin\{equation\}\\label\{2.17\}\
\\mathbf\{z\}^T(\\hat\{A\} \\mathbf\{y\}+\\mathbf\{v\})>c>0 \\text \{ for all \} \\mathbf\{y\} \\in \\Delta_n \\text \{ and \} \\mathbf\{v\} \\geq \\mathbf\{0\}.\
\\end\{equation\}\
We now assert that $\\mathbf\{z\} \\geq \\mathbf\{0\}$. Should this not be the case and $z_j<0$ for any $j$, we can consider a $\\mathbf\{v\}$ in $\\mathbb\{R\}^m$ where $v_j$ is sufficiently large and $v_i=0$ for all $i \\neq j$.\
This means that we would obtain a contradiction since $\\mathbf\{z\}^T(\\hat\{A\} \\mathbf\{y\}+\\mathbf\{v\}) = \\mathbf\{z\}^T \\hat\{A\} \\mathbf\{y\} + z_j v_j <$ 0 for some $\\mathbf\{y\} \\in \\Delta_n$. This contradicts the inequality from equation \\ref\{2.17\}.\
\
From inequality \\ref\{2.17\}, we can deduce that $\\mathbf\{z\}$ is not the zero vector. Consequently, the sum of its elements, represented as $s=\\sum_\{i=1\}^m z_i$, is strictly positive. It means that we can construct a vector $\\tilde\{\\mathbf\{x\}\}$ such that $\\tilde\{\\mathbf\{x\}\}=\\frac\{1\}\{s\}\\left(z_1, \\ldots, z_m\\right)^T=\\mathbf\{z\} / s$ belongs to $\\Delta_m$ and fulfills the inequality $\\tilde\{\\mathbf\{x\}\}^T \\hat\{A\} \\mathbf\{y\}>c / s>0$ for every $\\mathbf\{y\}$ in $\\Delta_n$.\
Therefore, it follows that $\\min_\{\\mathbf\{y\} \\in \\Delta_n\} \\tilde\{\\mathbf\{x\}\}^T A \\mathbf\{y\}>\\lambda$. This leads us to:\
\\begin\{equation\}\
\\max_\{\\mathbf\{x\} \\in \\Delta_m\} \\min_\{\\mathbf\{y\} \\in \\Delta_n\} \\mathbf\{x\}^T A \\mathbf\{y\}>\\lambda\
\\end\{equation\}\
Given that this holds for every $\\lambda$ that satisfies \\ref\{2.15\}, the theorem is established.\
\\end\{proof\}\
\\begin\{defi\}[Saddle Point and Pure Nash Equilibrium]\
    according to von Neumann's minimax theorem \\ref\{thm:vonneumann\}, there exist strategies $\\mathbf\{x\}$ and $\\mathbf\{y\}$ that satisfy \\begin\{equation\}\
V=\\text\{max\}_\{\\mathbf\{x\}\\in\\Delta_\{m\}\}\\text\{min\}_\{\\mathbf\{y\}\\in\\Delta_\{n\}\}\\mathbf\{x\}^\{T\}A\\mathbf\{y\}=\\text\{min\}_\{\\mathbf\{y\}\\in\\Delta_\{n\}\}\\text\{max\}_\{\\mathbf\{x\}\\in\\Delta_\{m\}\}\\mathbf\{x\}^\{T\}A\\mathbf\{y\}.\
\\end\{equation\} The paired strategies $(s_1^*, s_2^*)$ create a saddle point in the game, also known as a pure Nash equilibrium, because neither player benefits from changing their strategy.\
\\end\{defi\}\
\
\\subsubsection\{General Sum Game\}\
\
\\begin\{ex\}[Prisoner's Dilemma]\
    The prisoner's dilemma embodies a quintessential illustration of a scenario within game theory. The term "Prisoner's Dilemma" was later coined by Albert W. Tucker, who formalized the game with a story of two prisoners facing the decision to betray each other or remain silent \\citep*\{tucker1983mathematics\} and more information about the history of prisoner's dilemma can be found in the introduction at \\citep*\{RIOS2015930\}. It explicates the plausible rationale for two rational beings' failure to cooperate, despite the ostensible advantage in doing so. This dilemma constitutes a non-zero-sum game, where the total cumulation of player gains and losses can exceed or fall below zero. The game engages two players who invariably have a choice \'97 to either cooperate or defect. Mutual cooperation reaps a moderate reward for both players, whereas unilateral defection, with the other player choosing cooperation, results in a greater reward for the defector, and a lesser one for the cooperator. Similarly, both players opting for defection will lead to the receipt of meager rewards. The crux of this dilemma stems from a paradox; although both players benefit more from mutual cooperation, each may choose to defect in their pursuit of an individual advantage, ending up with lesser rewards than they could gain from mutual cooperation.\
The following matrix, as referenced in Table \\ref\{tab:perisoner\}, delineates the payoff structure: The negative numbers within the matrix signify years of incarceration. For instance, the pair (0,-10) implies that while prisoner 1 is acquitted, prisoner 2 receives a sentence of 10 years.\
\\begin\{table\}[!ht]\
    \\centering\
    \\begin\{tabular\}\{cc|c|c|\}\
         &  & Prisoner 2 & \\\\\
         &  & silence & confession\\\\\
         \\midrule %[2pt]\
        Prisoner 1 & silence & (-1,-1) & (-10,0))\\\\\
         & confession & (0,-10) & (-8,-8))\\\\\
    \\end\{tabular\}\
    \\caption\{Prisoner\'92s Dilemm\}\
    \\label\{tab:perisoner\}\
\\end\{table\}\
In the context of this game, prisoners are better positioned if they opt for silence over confession. However, prisoners choose their actions individually, and the optimal action for one prisoner, irrespective of the other's choice, is to confess; thereby indicating that confession constitutes a dominant strategy.\
\\end\{ex\}\
\
\
\
Drawing parallels to the zero-sum game scenario, one can define the payoff matrix for the general-sum game in a similar fashion:\
\\begin\{defi\}[Payoff Matrix of General Sum game]\
    In a two-person general-sum game, the game can be represented by a pair of $m \\times n$ payoff matrices, $A=\\left(a_\{i j\}\\right)$ and $B=\\left(b_\{i j\}\\right)$. The rows of these matrices are indexed by the $m$ potential actions of Player 1, and their columns are indexed by the $n$ potential actions of Player 2. Player 1 selects an action $i$ and Player 2 selects an action $j$, each oblivious to the other's choice. Following the revelation of these choices, Player 1 receives a payoff of $a_\{i j\}$, while Player 2 receives a payoff of $b_\{i j\}$.\
\\end\{defi\}\
\
\
\\begin\{defi\}[Mixed Strategy of General Sum Game]\
    A mixed strategy for player I is denoted by a vector $\\left(x_1, \\ldots, x_m\\right)^T$, wherein $x_i$ signifies the probability that player I initiates action $i$. Analogously, a mixed strategy for player II is outlined by a vector $\\left(y_1, \\ldots, y_n\\right)^T$, with $y_j$ denoting the probability that player II executes action $j$. A special case of mixed strategy, where a particular action is carried out with a probability of 1, is labelled as a pure strategy.\
\\end\{defi\}\
\\begin\{defi\}[Nash equilibrium]\\label\{def:nashgeneral\}\
    A mixed strategy pair denoted by $\\left(\\mathbf\{x\}^*, \\mathbf\{y\}^*\\right)$, with $\\mathbf\{x\}^* \\in \\Delta_m$ (where $\\Delta_m=\\left\\\{\\mathbf\{x\} \\in \\mathbb\{R\}^m: x_i \\geq 0, \\sum_\{i=1\}^m x_i=1\\right\\\}$ similarly as definition \\ref\{def:mixed\}) and $\\mathbf\{y\}^* \\in \\Delta_n$, is regarded as a Nash equilibrium if no player can obtain a higher payoff by unilaterally deviating from this strategy. This implies that $$\
\\left(\\mathbf\{x\}^*\\right)^T A \\mathbf\{y\}^* \\geq \\mathbf\{x\}^T A \\mathbf\{y\}^*\
$$ for every $\\mathbf\{x\} \\in \\Delta_m$, and $$\
\\left(\\mathbf\{x\}^*\\right)^T B \\mathbf\{y\}^* \\geq\\left(\\mathbf\{x\}^*\\right)^T B \\mathbf\{y\}\
$$ for every $\\mathbf\{y\} \\in \\Delta_n$.\
\
Moreover, $\\Delta_m$ is called a simplex. \
\\end\{defi\}\
\\begin\{defi\}[Symmetric Game]\
    A game is designated as symmetric if $m=n$ and the condition $a_\{i, j\} = b_\{j, i\}$ holds true for all $i, j \\in\\\{1,2, \\ldots, n\\\}$. A strategy pair $(\\mathbf\{x\}, \\mathbf\{y\})$ residing in $\\Delta_n$ is called symmetric if $x_i = y_i$ for all $i = 1, \\ldots, n$.\
\\end\{defi\}\
\\begin\{rem\}\
    One of the reasons why Nash equilibria are significant is that any strategy profile that doesn't fit into a Nash equilibrium is inherently unstable. This is because, by definition \\ref\{def:nashgeneral\}, there's bound to be at least one player who would choose to switch strategies. While a Nash equilibrium is guaranteed to exist, there can be multiple of them leading to different payoffs for the players. Consequently, Nash equilibria do not hold the same predictive power in general-sum games as safety strategies possess in zero-sum games. More critiques of Nash equilibria can be found in the appended notes.\
\\end\{rem\}\
Now, we can attempt to extend these definitions to a general-sum game scenario with more than two players:\
\\begin\{ex\}[Pok\'e9mon and Warhammer 40,000]\
    Games such as Pok\'e9mon and Warhammer 40,000 involve players strategically forming teams or armies of diverse characters or units, each possessing unique strengths, vulnerabilities, and abilities. These games can be balanced using game-theoretic approaches that modify zero-sum games with one variable per strategy, so that every strategy has an incentive to be employed \\citep*\{Liu2021BalancingZG\}. The complexity surpasses that of the Prisoner's Dilemma, given the presence of multiple variables contingent on varied strategies, thus necessitating a more complicated payoff matrix. The goal is to vanquish the opponent's contingent through strategic decisions and combinations of units or characters. Applicable to such games, game theory serves to analyze and equilibrate the available strategies. Through contemplating every potential choice and resultant outcome for each player, game theorists can pinpoint dominant strategies, equilibrium points, and prospective imbalances in the game design. For instance, in Pok\'e9mon, a particular Pok\'e9mon's type (such as fire, water, or grass) dictates its strengths and weaknesses vis-\'e0-vis other types. Game theory can guide the analysis of interactions among diverse types to prevent a single type from achieving overwhelming dominance, hence fostering a more balanced and enjoyable gaming experience. Alike in Warhammer 40,000, where players assemble armies comprising various distinctive abilities and characteristic units, game theory can discern dominant strategies or unit combinations potentially leading to imbalanced gameplay. This insight allows game designers to tailor the rules or change characteristics of units to endorse a more balanced and competitive environment. In both scenarios, game theory assists in modifying zero-sum games with single-variable strategies to ensure every strategy provides an incentive for usage, thus engendering a more dynamic gameplay experience. \
\\end\{ex\}\
We will now expand the definition of the Nash equilibrium from a two-player general-sum game to a game with more than two players:\
\
\\begin\{defi\}\
    A pure Nash equilibrium in a $k$-player game is represented by a family of pure strategies $\\left(s_1^*, \\ldots, s_k^*\\right) \\in S_1 \\times \\cdots \\times S_k$ such that, for each player $j \\in\\\{1, \\ldots, k\\\}$ and each $s_j \\in S_j$, $$\
u_j\\left(s_j^*, \\mathbf\{s\}_\{-j\}^*\\right) \\geq u_j\\left(s_j, \\mathbf\{s\}_\{-j\}^*\\right) .\
$$ In simpler terms, for every player $j$, their chosen strategy $s_j^*$ is the best response to the selected strategies $\\mathbf\{s\}_\{-j\}^*$ of the other players.\
\\end\{defi\}\
\\begin\{defi\}\
    A (mixed) strategy profile in a $k$-player game is a sequence $\\left(\\mathbf\{x\}_1, \\ldots, \\mathbf\{x\}_k\\right)$, where $\\mathbf\{x\}_j \\in \\Delta_\{\\left|S_j\\right|\}$ indicates a mixed strategy chosen by player $j$.\
\
A mixed Nash equilibrium refers to a strategy profile $\\left(\\mathbf\{x\}_1^*, \\ldots, \\mathbf\{x\}_k^*\\right)$, where for each player $j \\in\\\{1, \\ldots, k\\\}$ and every probability vector $\\mathbf\{x\}_j \\in \\Delta_\{\\left|S_j\\right|\}$, the following inequality holds true: $$\
u_j\\left(\\mathbf\{x\}_j^*, \\mathbf\{x\}_\{-j\}^*\\right) \\geq u_j\\left(\\mathbf\{x\}_j, \\mathbf\{x\}_\{-j\}^*\\right).\
$$ Here, the payoff function is given by $$\
u_j\\left(\\mathbf\{x\}_1, \\mathbf\{x\}_2, \\ldots, \\mathbf\{x\}_k\\right):=\\sum_\{s_1 \\in S_1, \\ldots, s_k \\in S_k\} \\mathbf\{x\}_1\\left(s_1\\right) \\cdots \\mathbf\{x\}_k\\left(s_k\\right) u_j\\left(s_1, \\ldots, s_k\\right),\
$$ where $\\mathbf\{x\}_i(s)$ symbolizes the probability that player $i$ assigns to the pure strategy $s$ in the mixed strategy $\\mathbf\{x\}_i$.\
\\end\{defi\}\
\
The most important result of this section:\
\\begin\{thm\}[Nash\'92s Theorem]\
   Every finite general-sum game has a\
Nash equilibrium.\
\\end\{thm\}\
\\begin\{rem\}\
As we progress towards Section \\ref\{section3\}, our discussion will extend to delve deeper into the proof of the Nash Equilibrium Theorem.\
\\end\{rem\}\
\
\
\
\
\\subsubsection\{Example 3 - Games with Perfect Information or Imperfect Information - Tic-Tac-Toe and matching pennies\}\
Consider a game of Tic-Tac-Toe played between two players. Each player takes turns placing their symbol (X or O) on a $3\\times3$ grid until one player achieves a winning combination of three symbols in a row, column, or diagonal, or until the grid is completely filled. This game can be classified as a game with perfect information, as each player can fully observe the state of the game at any given point.\
\
The game of Tic-Tac-Toe possesses a Nash equilibrium, which occurs when both players make optimal moves based on their opponent's moves. As an illustration, if player 1 starts by placing an X in the central square, player 2 can respond by placing an O in one of the corners. If player 1 then places an X in a corner opposite to the initial O, player 2 can reply by placing an O in the remaining corner. This leads to a situation where neither player can secure a win, resulting in a draw.\
\
Therefore, in the game of Tic-Tac-Toe, the Nash equilibrium is reached when both players employ optimal strategies, ultimately leading to a draw.\
\
In contrast, consider the game "matching pennies," where two players simultaneously choose to reveal either heads or tails on a penny. After revealing their choices, if the symbols match, player 1 wins; otherwise, player 2 wins. This game falls under the category of imperfect information, as each player lacks knowledge of the opponent's choice.\
\
The game of matching pennies features a Nash equilibrium when both players randomly choose their strategies with equal probability. For instance, if player 1 selects heads with a probability of 0.5 and tails with a probability of 0.5, player 2 can respond by selecting heads with a probability of 0.5 and tails with a probability of 0.5. This results in a scenario where neither player can gain an advantage by altering their strategy, leading to an expected payoff of zero for both players.\
\
Hence, in the game of matching pennies, the Nash equilibrium is achieved when both players randomly choose their strategies with equal probability, resulting in an expected payoff of zero for both players.\
\\subsection\{Nash equilibria\}\\label\{section3\}\
This section is structured to first address a historical overview of the Nash Equilibrium Theorem. We will trace its evolution over time, from its inception by John Nash to various developments it underwent in the proof formulation. This historical journey will not only offer detailed insights into the theorem's significance in game theory but also clarify our approach towards its proof.\
Additionally, the section will highlight the key references and sources that have guided our study of the Nash Equilibrium Theorem. These resources have been instrumental in shaping our understanding as well as our derivation and interpretation of the proof.\
\\subsubsection\{History and references\}\
John Nash pioneered the concept that is now famously known as the "Nash equilibrium" in the 1950s \\citep*\{nash1950equilibrium,nash1951non\}. With no exaggeration, this is the most impactful concept in game theory to this day. Nash's original proof utilizes Kakutani's fixed point theorem \\citep*\{nash1950equilibrium\}. In relation to this, our proof of Theorem \\ref\{theorem23\} builds on the method presented in a subsequent publication by Nash \\citep*\{nash1951non\}.\
We also acknowledge and utilize the contributions made by Sperner, whose Lemma \\ref\{sperner's lemma\} is from 1928, and Brouwer, whose Theorem \\ref\{Brouwersimplex\} was published in 1912. Our approach to verifying these theories is substantially influenced by Border's work \\citep*\{border1985fixed\}.\
Please note that this document forms a part of our larger work \\citep*\{shoham2008multiagent\}. Through this, we are exploring the significance and applications of these principles further.\
\\subsubsection\{Nash's equilibria for two players\}\
Key to demonstrating the existence of Nash Equilibria via mixed strategies is the prior establishment of the Brouwer Fixed Point Theorem. This theorem serves as a cornerstone underpinning the majority of proofs validating the existence of equilibria in game theory scenarios.\
\\begin\{thm\}[Brouwer Fixed Point Theorem]\
Given that $S \\subset \\mathbb\{R\}^n$ is convex and compact, if $T: S \\rightarrow S$ is a continuous function, then there exists a fixed point. i.e., there is an $x^* \\in S$ for which $T\\left(x^*\\right)=x^*$.\
\\end\{thm\}\
\
The proof for the general case will not be discussed here. However, the one-dimensional case is significantly simpler.  \
\\begin\{proof\}\
    Consider the real line $\\mathbb\{R\}$ when $n=1$. In this case, compact and convex sets are limited to closed intervals denoted by $[a, b]$. Let us denote the continuous function as $T:[a, b] \\rightarrow[a, b]$. Fixed point cases can be observed clearly when $T(a)=a$ or $T(b)=b$.\
\
Alternatively, in instances where $T(a)>a$ and $T(b)<b$, we propose a new function, $g(x)$, defined as $g(x)=T(x)-x$. In this instance, we observe that $g(a)>0$ and $g(b)<0$. Given the continuity of $T$, function $g$ also exhibits continuity.\
\
Employing the Intermediate Value Theorem, we presume the existence of a certain $x^*\\in(a, b)$ such that $g(x^*)=0$. This $x^*$ hence becomes the validated fixed point of the function $T$. \
\\end\{proof\}\
\\begin\{rem\}\
Indeed, the elegant and succinct proof in the case of dimension one \
$\\mathbb\{R\}^\{1\}$\
 provides a useful foundation, but it's worth noting that proving the Brouwer fixed point theorem in a higher-dimensional context can be considerably more intricate. To maintain the conceptual continuity and readability within our discussion pivoting around the primary theorem, the Nash Equilibrium Theorem, we've chosen to present the proof of the Brouwer Fixed Point Theorem subsequent to detailing the proof for the Nash Equilibrium Theorem. For a more comprehensive and detailed derivation of the Brouwer Fixed Point Theorem general case, one can refer to the book \\citep*\{karlin2017game\}. \
\\end\{rem\}\
\
Now we are ready to prove the Nash's equilibria theorem:\
\
\\begin\{thm\}[Nash\'92s Theorem]\
 For any general-sum finite game with $k \\geq 2$ players, there exists at least one Nash equilibrium.\
\\end\{thm\}\
\
\\begin\{proof\}\
\\textbf\{Nash's Theorem for two players via The Brouwer Fixed Point Theorem\}:\
Our analysis begins with the two-player scenario. Let's assume that the game is characterized by payoff matrices $A_\{m \\times n\}$ and $B_\{m \\times n\}$ for players I and II, respectively. Define $K$ as the product of two simplices, $K=\\Delta_m \\times \\Delta_n$.\
\
We aim to determine a continuous function $T: K \\rightarrow K$, which maps a strategy pair, denoted as $(\\mathbf\{x\}, \\mathbf\{y\})$, to a new strategy pair represented as $(\\hat\{\\mathbf\{x\}\}, \\hat\{\\mathbf\{y\}\})$, satisfying the below conditions:\
\
\\begin\{enumerate\}\
    \\item[i] If there exists a strategy $\\hat\{\\mathbf\{x\}\}$ that is a better response to strategy $\\mathbf\{y\}$ than $\\mathbf\{x\}$, then $\\hat\{\\mathbf\{x\}\}$ is selected; in the absence of such a response, we keep $\\hat\{\\mathbf\{x\}\}=\\mathbf\{x\}$. \
    \\item[ii] Furthermore, if there exists a strategy $\\hat\{\\mathbf\{y\}\}$ that provides a better response to strategy $\\mathbf\{x\}$ than $\\mathbf\{y\}$, then we select $\\hat\{\\mathbf\{y\}\}$; otherwise, we maintain $\\hat\{\\mathbf\{y\}\}=\\mathbf\{y\}$.\
\\end\{enumerate\}\
\
\
In accordance with these conditions, any fixed point of function $T$ qualifies as a Nash Equilibrium. The proof then follows by demonstrating the existence of a fixed point using Brouwer fixed point theorem.\
\
Let's fix the strategy $\\mathbf\{y\}$ of player II. We then define $c_i$ to be the positive part of the potential gain obtained by player I when switching from the mixed strategy $\\mathbf\{x\}$ to the pure strategy $i$. Formally, for any $\\mathbf\{x\} \\in \\Delta_m$, we have $c_i$ defined as:\
\
$$\
c_i:=c_i(\\mathbf\{x\}, \\mathbf\{y\}):=\\max \\left\\\{A_i \\mathbf\{y\}-\\mathbf\{x\}^T A \\mathbf\{y\}, 0\\right\\\},\
$$ where $A_i$ represents the $i^\{\\text\{th\}\}$ row of the matrix $A$.\
\
Afterwards, we identify the new mixed strategy $\\hat\{\\mathbf\{x\}\} \\in \\Delta_m$ for player I by $$\
\\hat\{x\}_i:=\\frac\{x_i+c_i\}\{1+\\sum_\{k=1\}^m c_k\},\
$$ which indicates that each pure strategy of player I is assigned a weight incremented proportionally to its relative efficiency against the mixed strategy $\\mathbf\{y\}$ of player II.\
\
Advancing in a similar idea, we define:\
\
$$\
d_j:=d_j(\\mathbf\{x\}, \\mathbf\{y\}):=\\max \\left\\\{\\mathbf\{x\}^T B^j-\\mathbf\{x\}^T B \\mathbf\{y\}, 0\\right\\\},\
$$ where $B^j$ represents the $j^\{\\text\{th\}\}$ column of $B$.\
\
We subsequently derive player II's new mixed strategy $\\hat\{\\mathbf\{y\}\} \\in \\Delta_n$ under the formula:\
\
$$\
\\hat\{y\}_j:=\\frac\{y_j+d_j\}\{1+\\sum_\{k=1\}^n d_k\}.\
$$ This expression allows us to calculate the new weight for each pure strategy of player II given $\\mathbf\{x\}$, the mixed strategy of player I.\
\
Ultimately, we denote the function mapping the strategy pair to the new pair as $T(\\mathbf\{x\}, \\mathbf\{y\})=(\\hat\{\\mathbf\{x\}\}, \\hat\{\\mathbf\{y\}\})$.\
\
We assert that property (i) is satisfied for this proposed mapping. Specifically, in scenarios where $c_i=0$ for all $i$ (equivalently $\\mathbf\{x\}^T A \\mathbf\{y\} \\geq A_i \\mathbf\{y\}$), then $\\hat\{\\mathbf\{x\}\}=\\mathbf\{x\}$ stands as a best response to $\\mathbf\{y\}$. Alternatively, in circumstances where at least one $c_i>0$, we denote $S:=\\sum_\{i=1\}^m c_i$. This implies that we need to demonstrate that:\
\
$$\
\\sum_\{i=1\}^m \\hat\{x\}_i A_i \\mathbf\{y\}>\\mathbf\{x\}^T A \\mathbf\{y\}.\
$$ When both sides of the inequality are multiplied by $(1+S)$, the above equivalence is transformed into: $$\
\\sum_\{i=1\}^m\\left(x_i+c_i\\right) A_i \\mathbf\{y\}>(1+S) \\mathbf\{x\}^T A \\mathbf\{y\}.\
$$ This form holds true since we can deduce that: $$\
\\sum_\{i=1\}^m c_i A_i \\mathbf\{y\}>S \\mathbf\{x\}^T A \\mathbf\{y\}=\\sum_i c_i \\mathbf\{x\}^T A \\mathbf\{y\}.\
$$ By following a parallel logic, we can affirm that property (ii) is also satisfied. Hence, consistent with both conditions, the defined mapping qualifies for substantiating fixed points as Nash Equilibria.\
\
Lastly, we note that $K$ stands as a convex, compact (both closed and bounded) set, and function $T$ is continuous in nature. The continuity of $T$ is guaranteed by the continuous characteristics of $c_i$ and $d_j$. The existence of a fixed point $(\\mathbf\{x\}, \\mathbf\{y\}) \\in K$ satisfying $T(\\mathbf\{x\}, \\mathbf\{y\})=(\\mathbf\{x\}, \\mathbf\{y\})$ is hence concluded directly by the application of Brouwer's Fixed Point Theorem. Given properties (i) and (ii), this fixed point $(\\mathbf\{x\}, \\mathbf\{y\})$ verifies a Nash Equilibrium.\
\
In cases featuring more than two players, i.e., $k>2$, we proceed by defining a quantity $c_\{\\ell\}^\{(j)\}$ for every player $j$ and their pure strategy $\\ell$. This quantity represents the (positive) change in the payoff of player $j$ on switching from their existing strategy $\\mathbf\{x\}^\{(j)\}$ to the pure strategy $\\ell$, while the strategies of all other players are held constant. With this defined, the rest of the argument and proof follow the same course, thus generalizing the existence of Nash Equilibria to games of $k>2$ players.\
\\end\{proof\}\
\\subsubsection\{Nash's equilibria for high dimension\}\
\\begin\{thm\}[Brouwer's fixed point theorem for higher dimension]\\label\{Brouwersimplex\}\
\
Let $f: \\triangle_m \\rightarrow \\triangle_m$ be continuous. Then $f$ has a fixed point-that is, there exists some $z \\in \\triangle_m$ such that $f(z)=z$.\
\\end\{thm\}\
\
In order to prove this, we need to review some definitions from analysis and geometry:\
\
\\begin\{defi\}[ $n$-simplex]\
 An $n$-simplex, denoted $x^0 \\cdots x^n$, is the set of all convex combinations of the affinely independent set of vectors $\\left\\\{x^0, \\ldots, x^n\\right\\\}$, i.e.\
$$\
\\Delta_\{n\}=\\left\\\{\\sum_\{i=0\}^n \\lambda_i x^i: \\forall i \\in\\\{0, \\ldots, n\\\}, \\lambda_i \\geq 0 ; \\text \{ and \} \\sum_\{i=0\}^n \\lambda_i=1\\right\\\} .\
$$\
\
Each $x^i$ is called a vertex of the simplex $x^0 \\cdots x^n$ and each $k$-simplex $x^\{i_0\} \\cdots x^\{i_k\}$ is called a $k$-face of $x^0 \\cdots x^n$, where $i_0, \\ldots, i_k \\in\\\{0, \\ldots, n\\\}$.\
\\end\{defi\}\
\
\\begin\{defi\}[Standard $n$-simplex]\
The standard $n$-simplex $\\triangle_n$ is \
\\begin\{equation\}\
    \\begin\{array\}\{cc\}\
\\left\\\{y \\in \\mathbb\{R\}^\{n+1\}\\right. : \\left.\\sum_\{i=0\}^n y_i=1, \\forall i=0, \\ldots, n, y_i \\geq 0\\right\\\}.\
\\end\{array\}\
\\end\{equation\}\
\\end\{defi\}\
\
\\begin\{thm\}[Simplicial subdivision]\
A simplicial subdivision of an $n$-simplex $T$ is a finite set of simplexes $\\left\\\{T_i\\right\\\}$ for which $\\bigcup_\{T_i \\in T\} T_i=T$, and for any $T_i, T_j \\in T, T_i \\cap T_j$ is either empty or equal to a common face.\
    \
\\end\{thm\}\
\
Intuitively speaking, a simplex is partitioned into a collection of smaller simplices, each of which collectively occupies the same spatial region as the original simplex, with overlapping occurring strictly at their boundaries. Moreover, any intersection of two sub-simplices is expected to coincide with an entire face of both sub-simplices. For instance, a 2-simplex subdivided into 16 sub-simplices is illustrated in Figure \\ref\{fig:simplex\} (left).\
\
\\begin\{figure\}[!htp]\
    \\centering\
    \\includegraphics[width=0.9\\linewidth]\{simplex.png\}\
    \\caption\{: A properly labeled simplex (left), and the same simplex with completelylabeled subsimplexes shaded and three walks indicated (right).\}\
    \\label\{fig:simplex\}\
\\end\{figure\}\
\
Let's consider an arbitrary point $y$ in a simplex $x^0 \\cdots x^n$. This point can be expressed as a convex combination of the vertices of the simplex: $y=\\sum_i \\lambda_i x^i$. We then introduce $\\chi$ as a function mapping the point $y$ to the set of vertices contributing to it, formulated as: $\\chi(y)=\\left\\\{i: \\lambda_i>0\\right\\\}$. This function $\\chi$ is pivotal in facilitating a proper labeling of the vertices of the subdivided simplex.\
\
\
\\begin\{defi\}[Proper labeling]\
Let $T=x^0 \\cdots x^n$ be simplicially subdivided, and let $V$ denote the set of all distinct vertices of all the subsimplexes. A function $$\\mathcal\{L\}: V \\rightarrow\\\{0, \\ldots, n\\\}$$ is a proper labeling of a subdivision if $\\mathcal\{L\}(v) \\in \\chi(v)$.\
\\end\{defi\}\
\
Consistent with this definition, an inherent consequence is that all vertices of a simplex, being adjacent to each other, are obligated to have distinct labels. This requirement stems from the definition of proper labeling which necessitates distinct labels to adjacent vertices - vertices that are directly connected via an edge.\
\
Considering a simplex, this is an n-dimensional analogue of a triangle or tetrahedron, and has every vertex directly connected to every other vertex by an edge, hence all vertices become adjacent to each other.\
\
Consequently, abiding by the definition of proper labeling, the provision of the same label to any two vertices of the simplex is forbidden due to the adjacency of all vertices with each other. Non-compliance with this would breach the definition of proper labeling.\
\
For instance, the subdivisions of the simplex illustrated in Figure \\ref\{fig:simplex\} (left) provide an example of proper labeling.\
\
\\begin\{rem\}\
    Such a condition not only helps avoid confusion but also maintains a systematic structure within the labeling system, which is especially pertinent in studies involving topological or combinatorial objects. In such frameworks, labeling forms a critical aspect of comprehending the geometric or algebraic structures involved.\
\\end\{rem\}\
\
\\begin\{defi\}[complete labeling]\
A subsimplex is completely labeled if $\\mathcal\{L\}$ assumes all the values $0, \\ldots, n$ on its set of vertices.\
\
\
\\end\{defi\}\
For example in the subdivided triangle in Figure \\ref\{fig:simplex\} (left), the sub-triangle at the very top is completely labeled.\
\
\\begin\{lem\}[Sperner's lemma]\\label\{sperner's lemma\}\
Let $T_n=x^0 \\cdots x^n$ be simplicially subdivided and let $\\mathcal\{L\}$ be a proper labeling of the subdivision. Then there are an odd number of completely labeled subsimplexes in the subdivision.\
\\end\{lem\}\
\\begin\{proof\}\
Our goal is to establish this proposition via an inductive argument on $n$. The base case at $n=0$ is trivial. In this instance, the simplex is solely comprised of a single point $x^0$. The only simplicial subdivision possible in this case is $\\left\\\{x^0\\right\\\}$. The only possible labeling function could be $\\mathcal\{L\}(x^0)=0$, and observe that this indeed qualifies as a proper labeling. As a result, we have a single completely labeled sub-simplex, which is $x^0$ itself.\
\
Proceeding with the inductive step, we assume the proposition holds for $n-1$ and establish its validity for $n$. The simplicial subdivision of the n-simplex $T_n$ also results in a simplicial subdivision of its face $x^0 \\cdots x^\{n-1\}$. Observe that this face, denoted as $T_\{n-1\}$, is an $(n-1)$-simplex. The restriction of the labeling function $\\mathcal\{L\}$ to the face $T_\{n-1\}$ provides a proper labeling of the $(n-1)$-simplex. Hence, by our induction hypothesis, there exists an odd number of $(n-1)$-subsimplices within $T_\{n-1\}$ that bear proper labelings.\
\
To offer a clearer visual understanding that's beneficial for the inductive reasoning, let's explore the illustration of a subdivided 2-simplex. Observing Figure \\ref\{fig:simplex\} (left), note that the bottom face, consisting of the vertices $x^0$ and $x^1$, is a subdivided 1-simplex\'96essentially a line segment. Within this subspace, we can spot four sub-simplices, out of which three represent completely labeled sub-simplices.\
\
We outline a strategy for traversing or "walking" through the subdivided and labeled n-simplex, denoted as $T_n$. This walk commences at an $(n-1)$-subsimplex with labels $(0, \\ldots, n-1)$ located on the face $T_\{n-1\}$; let's denote this subsimplex as $b$. Uniquely attached to this $b$ is an n-subsimplex, named $d$, which contains $b$ as a face. The vertices of $d$ constitute the vertices of $b$ alongside another vertex $z$.\
\
If $z$ happens to be labeled $n$, then we find ourselves with a fully labeled subsimplex, which causes the walk to terminate. On the contrary, should $z$ carry a label other than $n$, the labels on $d$ fall in the sequence $(0, \\ldots, n-1)$, where one label (for instance, $j$) is repeated, and label $n$ is absent. Consequently, only one other $(n-1)$-subsimplex holds the labels $(0, \\ldots, n-1)$. This condition arises since every $(n-1)$-face of $d$ is defined by excluding one of $d$'s vertices. Given the repetition of label $j$ alone, an $(n-1)$-face of $d$ bears labels $(0, \\ldots, n-1)$ exclusively when one of the two vertices branded with label $j$ is omitted. We have initial face $b$ as one such face. Hence, exactly one other face fulfills this condition, which we denote as $e$.\
\
The subsequent course of our "walk" initiates from $e$, acknowledging that an $(n-1)$-face of an n-subsimplex in our subdivided simplex $T_n$ either resides on an $(n-1)$-face of $T_n$ or constitutes the intersection of two n-subsimplexes. Should $e$ lie on an $(n-1)$-face of $T_n$, the walk ceases. Conversely, if not, our walk extends into the singular other n-subsimplex sharing $e$ as a face. This new subsimplex will either be completely labeled or have an instance of a repeated label. Concurrently navigating this condition, we press on with the walk in a manner akin to our exploration of subsimplex $d$.\
\
Observe that the trajectory determined by a random walk is solely governed by the initial $(n-1)$-simplex. We specify that a random walk terminates once it reaches a fully labeled $n$-simplex or once it reaches a $(n-1)$-simplex with labels $(0, \\ldots, n-1)$ on the face $T_\{n-1\}$. Since $\\mathcal\{L\}$ is a proper labeling, a walk cannot terminate on any other face. Note further that this process is reversible; starting from an endpoint and following the described labeling rule in reverse will necessarily lead back to the walk's origin.\
\
Such reversibility implies that if a walk begins at some $t \\in T_\{n-1\}$ and concludes at some $t' \\in T_\{n-1\}$, then $t\\neq t'$. Should $t = t'$ occur, we would be able to iteratively reverse the walk, thereby finding an alternative trajectory originating from the same initial $(n-1)$-simplex. This outcome, however, contradicts the uniqueness of the walk. Figure \\ref\{fig:simplex\} (right) illustrates one walk of each of the\
kinds we have discussed so far: one that starts and ends at different subsimplexes\
on the face $x^\{0\}x^\{1\}$, and one that starts on the face $x^\{0\}x^\{1\}$ and ends at a completely\
labeled sub-triangle.\
\
By the inductive hypothesis, we have established that there is an odd number of $(n-1)$-simplices possessing the label series $(0, \\ldots ,n-1)$ on $T_\{n-1\}$. Therefore, there must necessarily exist at least one walk that does not terminate on this face. All walks where the start and end locations are $T_\{n-1\}$ are paired, and hence, there is an odd count of walks originating from $T_\{n-1\}$ that terminate on fully labeled simplices.\
\
Because each walk can only terminate once, it implies that all such walks must terminate at distinct fully labeled simplices, given that each fully labeled simplex contains exactly one $(n-1)$-simplex face labeled with $(0, \\ldots, n-1)$ and because each walk terminates when it reaches such a simplex.\
\
In conclusion, this rigorous relabeling process ensures a one-to-one correspondence with walks and fully labeled $n$-simplices.\
\
We cannot state that all completely labeled subsimplexes are necessarily obtained from the said walks. This can be verified by considering walks in reverse direction, commencing from completely labeled subsimplexes. While it is true that a portion of these reverse walks conclude at $(n-1)$-simplexes on $T_\{n-1\}$, it should also be noted that others may end at completely labeled $n$-subsimplexes (As illustrated in Figure \\ref\{fig:simplex\} (right), which is one such example). However, what these reverse walks essentially do is to establish a pairing of completely labeled subsimplexes. Naturally, this implies the number of completely labeled subsimplexes pairing up with each other has to be even. Additionally, there is an odd number of completely labeled subsimplexes that are not paired but instead, are obtained from walks commencing from the face $T_\{n-1\}$. Consequently, it can be concluded that the net number of completely labeled subsimplexes is odd.\
\\end\{proof\}\
\
\\begin\{lem\}\\label\{lem:cptsimplex\}\
    $\\Delta_\{m\}$ as a closed and bounded subset of $\\mathbb\{R\}^\{ n\}$ is compact.\
\\end\{lem\}\
\
By virtue of the compactness of $\\Delta_\{m\}$, we can infer that every sequence within it possesses a convergent subsequence.\
\
\\begin\{defi\}[Centroid]\
The centroid of a simplex $x^0 \\cdots x^m$ is the "average" of its vertices, $\\frac\{1\}\{m+1\} \\sum_\{i=0\}^m x^i$.\
\\end\{defi\}\
\
We have now prepared all the necessary groundwork to apply Sperner's Lemma in our proof of Brouwer's fixed point theorem \\ref\{Brouwersimplex\}.\
\
\\begin\{proof\}[Proof of Brouwer's fixed point theorem \\ref\{Brouwersimplex\}]\
Our proof hinges on two key steps. Firstly, we will construct a suitable labeling for $\\triangle_m$. Secondly, we will demonstrate that upon progressively refining the subdivisions, there exists a subsequence of entirely labeled subsimplexes that converges to a fixed point of the function $f$.\
\
\\textbf\{Part 1: \}$\\mathcal\{L\}$ \\textbf\{is a proper labeling.\} Let $\\epsilon>0$. We simplicially subdivide $\\triangle_m$ such that the Euclidean distance between any two points in the same $m$-subsimplex is at most $\\epsilon$. We define a labeling function $\\mathcal\{L\}: V \\rightarrow\\\{0, \\ldots, m\\\}$ as follows. For each $v$ we choose a label satisfying\
\\begin\{equation\}\\label\{eq1\}\
    \\mathcal\{L\}(v) \\in \\chi(v) \\cap\\left\\\{i: f_i(v) \\leq v_i\\right\\\},\
\\end\{equation\}\
where $v_i$ is the $i^\{\\text \{th \}\}$ component of $v$ and $f_i(v)$ is the $i^\{\\text \{th \}\}$ component of $f(v)$. In other words, $\\mathcal\{L\}(v)$ can be any label $i$ such that $v_i>0$ and $f$ weakly decreases the $i^\{t \\text \{ th \}\}$ component of $v$. \
\
As a precursor to confirm that$\\mathcal\{L\}$ is well-defined, we are required to verify the right side of Equation \\ref\{eq1\} has a non-empty intersection. Intuitively, since both $v$ and $f(v)$ reside on the standard simplex $\\triangle_m$, where the sum of components for each point equals 1 on $\\triangle_m$, a component of $v$ that is weakly decreased by $f$ naturally exists. This principle remains intact even when we limit to the components in $\\chi(v)$, which are precisely the positive components of $v$. To showcase the above formally, let us consider the converse for contradiction. Going by this assumption, we have $f_i(v)>v_i$ for all $i \\in \\chi(v)$. Recalling the characteristics of a standard simplex, the equality $\\sum_\{i=0\}^m v_i=1$ holds true. Given the definition of $\\chi$, we have $v_j>0$ if and only if $j \\in \\chi(v)$. Thus, upon equating we arrive at, \
\\begin\{equation\}\\label\{eq2\}\
    \\sum_\{j \\in \\chi(v)\} v_j=\\sum_\{i=0\}^m v_i=1.\
\\end\{equation\}\
\
Given that $f_j(v)>v_j$ for all $j \\in \\chi(v)$, we have \
\\begin\{equation\}\\label\{eq3\}\
\\sum_\{j \\in \\chi(v)\} f_i(v)>\\sum_\{j \\in \\chi(v)\} v_j=1 .\
\\end\{equation\}\
Simultaneously, as $f(v)$ resides on the standard simplex $\\triangle_m$ as well, we obtain \
\\begin\{equation\}\\label\{eq4\}\
\\sum_\{j \\in \\chi(v)\} f_i(v) \\leq \\sum_\{i=0\}^m f_i(v)=1 .\
\\end\{equation\}\
This simultaneously leads to a contradiction given that equations \\ref\{eq3\} and \\ref\{eq4\} contradict each other. Hence, the proof holds that $\\mathcal\{L\}$ is well-defined and proper labelling from construction.\
\
\\textbf\{Part 2:\} We now turn our attention to the convergence of completely labeled subsimplexes to fixed points of $f$ as $\\epsilon \\rightarrow 0$. Given that $\\mathcal\{L\}$ is a proper labeling, it follows from Sperner's Lemma \\ref\{sperner's lemma\} that there exists at least one completely labeled subsimplex $p^0 \\cdots p^m$ such that $f_i\\left(p^i\\right) \\leq p^i$ for each $i=1,\\ldots,m$. As $\\epsilon \\rightarrow 0$, consider the sequence of centroids of completely labeled subsimplexes. Due to the compactness of $\\triangle_m$ \\ref\{lem:cptsimplex\}, this sequence gives rise to a convergent subsequence. We denote its limit by $z$; by the continuity of $f$, as $\\epsilon \\rightarrow 0$, we have $f_i(z) \\leq z_i$ for all $i=1,\\ldots,m$ and $p^i \\rightarrow z$. This implies $f(z)=z$, because if it was not the case, we could use a similar argument to that of Part 1 and we would find that $1=\\sum_i f_i(z)<\\sum_i z_i=1$ - this is a contradiction. Hence, confirming that the completely labeled subsimplexes converge to fixed points of $f$ as $\\epsilon \\rightarrow 0$.\
\\end\{proof\}\
\
Theorem \\ref\{Brouwersimplex\} does not directly deduce to the proof concerning the existence of Nash equilibria. This limitation arises from the fact that a Nash equilibrium can be modelled as a point belonging to the set of mixed strategy profiles, denoted by $S$. Interestingly, this set does not form a simplex but configures into a simplotope - a Cartesian product of simplexes. Here it's important to note that the mixed strategy of each individual agent can be comprehensively represented as a point within a simplex. Nevertheless, it has been established that Brouwer's theorem can indeed be expanded to accommodate simplotopes in addition to simplexes.An argument similar to our proof below can be used to prove a generalization of Theorem \\ref\{Brouwersimplex\} to arbitrary\
convex and compact sets. Fundamentally, this extension is viable since a simplotope bears a topological resemblance to a simplex. \
\
\\begin\{coro\}[Brouwer's fixed point theorem, simplotopes]\\label\{coro\}\
Let $K=\\prod_\{j=1\}^k \\triangle_\{m_j\}$ be a simplotope and let $f: K \\rightarrow K$ be continuous. Then $f$ has a fixed point.\
\\end\{coro\}\
\
\
\\begin\{proof\}\
Let $m=\\sum_\{j=1\}^k m_j$. We now aim to demonstrate that a continuous function $f: K \\rightarrow K$ possesses a fixed point, given the condition that $K$ is homeomorphic to $\\triangle_m$. Consider that $h: \\triangle_m \\rightarrow K$ is a homeomorphism. Consequently, we can express $h^\{-1\} \\circ f \\circ h: \\triangle_m \\rightarrow \\triangle_m$ as a continuous function, where $\\circ$ denotes function composition. As per Theorem 18, there exists a $z^\{\\prime\}$ such that $h^\{-1\} \\circ f \\circ h\\left(z^\{\\prime\}\\right) =$ $z^\{\\prime\}$. Choosing $z=h\\left(z^\{\\prime\}\\right)$, we arrive at $h^\{-1\} \\circ f(z)=z^\{\\prime\}=h^\{-1\}(z)$. Given the injective nature of $h^\{-1\}$, it necessarily follows that $f(z)=z$.\
We now need to ascertain that $K=\\prod_\{j=1\}^k \\triangle_\{m_j\}$ is homeomorphic to $\\triangle_m . K$'s status as compact and convex derives from the compactness and convexity of all $\\triangle_\{m_j\}$, since the Cartesian product of such sets maintains these properties. For the following argument, let the dimension of a subset of an Euclidean space be the count of independent parameters needed to capture each point in the set. As an example, the dimension of an $n$-simplex is $n$. Given each $\\triangle_\{m_j\}$'s dimension is $m_j$, $K$, in turn, renders a dimension of $m$. Considering $K \\subset \\mathbb\{R\}^\{m+k\}$ and $\\triangle_m \\subset \\mathbb\{R\}^\{m+1\}$ both display a dimension of $m$, they can be correspondingly embedded in $\\mathbb\{R\}^m$ as $K^\{\\prime\}$ and $\\triangle_m^\{\\prime\}$. Notably, while $K \\subset \\mathbb\{R\}^\{m+k\}$ and $\\triangle_m \\subset \\mathbb\{R\}^\{m+1\}$ lack interior points, both $K^\{\\prime\}$ and $\\triangle_m^\{\\prime\}$ exhibit non-empty interiors. As an example, a standard 2-simplex can be situated within $\\mathbb\{R\}^3$, but we can embed the triangle in $\\mathbb\{R\}^2$, as readily depicted in Figure \\ref\{fig:2\} (left), where the product of two standard 1-simplexes results in a square which can similarly be embedded in $\\mathbb\{R\}$. In order to transform $K^\{\\prime\}$ into $K^\{\\prime \\prime\}$ such that $K^\{\\prime \\prime\}$ resides strictly inside $\\triangle_m^\{\\prime\}$, we scale and translate it. Given that both scaling and translation are homeomorphisms, and a composition of homeomorphisms remains a homeomorphism, all we need is to establish a homeomorphism $h: K^\{\\prime \\prime\} \\rightarrow \\triangle_m^\{\\prime\}$. As a final step, we fix a point $a$ within the interior of $K^\{\\prime \\prime\}$.\
\\begin\{figure\}[!htp]\
    \\centering\
    \\includegraphics[width=0.95\\linewidth]\{fig2.png\}\
    \\caption\{A product of two standard 1-simplexes is a square (a simplotope; left). The\
square is scaled and put inside a triangle (a 2-simplex), and an example of radial projection h is shown (right).\}\
    \\label\{fig:2\}\
\\end\{figure\}\
Denote our homeomorphism as being the 'radial projection' relative to $a$: that is, $h(a)=a$, and for all $x \\in K^\{\\prime \\prime\} \\setminus \\\{a\\\}$, our homeomorphism is then described by $$\
h(x) = a + \\frac\{\\left\\|x^\{\\prime\}-a\\right\\|\}\{\\left\\|x^\{\\prime \\prime\}-a\\right\\|\} (x-a).\
$$ Here, $x^\{\\prime\}$ designates the intersection point of the $\\triangle_m^\{\\prime\}$ boundary with the ray initiated at $a$ and crossing through $x$, whilst $x^\{\\prime \\prime\}$ denotes the intersection point of the $K^\{\\prime \\prime\}$ boundary with that same ray. On account of $K^\{\\prime \\prime\}$ and $\\triangle_m^\{\\prime\}$ possessing both convexity and compactness, $x^\{\\prime \\prime\}$ and $x^\{\\prime\}$ are necessarily existent and unique. Additionally, as $a$ is situated within the interior of $K^\{\\prime \\prime\}$ and $\\triangle_m,\\left\\|x^\{\\prime\}-a\\right\\|$ and $\\left\\|x^\{\\prime \\prime\}-a\\right\\|$ are guaranteed to be positive. Intuition suggests that $h$ scales $x$ along the ray by a factor of $\\frac\{\\left\\|x^\{\\prime\}-a\\right\\|\}\{\\left\\|x^\{\\prime \\prime\}-a\\right\\|\}$. Figure \\ref\{fig:2\} (right) provides a visual representation of this radial projection from a square simplotope towards a triangle.\
\
We are now left with the task of demonstrating that $h$ is a homeomorphism. The continuity of $h$ is fairly self-evident. Noting the fact that $h(x)$ is situated on the ray starting at $a$ and running through $x$, we can recreate the identical ray by drawing a line from $a$ and extending it through $h(x)$. This enables us to retrieve $x^\{\\prime\}$ and $x^\{\\prime \\prime\}$ and thus determine $x$ via scaling $h(x)$ along the ray by a factor of $\\frac\{\\left\\|x^\{\\prime \\prime\}-a\\right\\|\}\{\\left\\|x^\{\\prime\}-a\\right\\|\}$. It follows that $h$ is injective. To show that $h$ is onto, we can apply the following logic: for any given point $y \\in \\triangle_m^\{\\prime\}$, we can construct a ray and identify an $x$ such that $h(x)=y$ holds. Accordingly, $h^\{-1\}$ shares the same form as $h$, albeit with an inverted scaling factor, leading us to conclude that $h^\{-1\}$ is continuous, and hence, that $h$ serves as a homeomorphism.\
\\end\{proof\}\
\
\
\
We are now poised to validate the existence of a Nash equilibrium. Having established Corollary \\ref\{coro\} and defined notation pertinent to mixed strategies \\ref\{def:mixed\}, the simplicity of the ensuing proof might come as a surprise. The procedure of this proof involves crafting a continuous function $f: S \\rightarrow S$ wherein each fixed point of $f$ corresponds to a Nash equilibrium. Subsequently, we use Corollary \\ref\{coro\} as a means to reason that $f$ has at least one fixed point, thus substantiating that Nash equilibria invariably exist.\
\
\\begin\{thm\}\\label\{theorem23\}\
    Every game with a finite number of players and action pro- files has at least one Nash equilibrium.\
\\end\{thm\}\
\
\\begin\{proof\}\
    \\textbf\{Nash's theorem for finite players via corollary \\ref\{coro\}\}\
\
Let's denote by $N$ the set of all players and by $A_i$ the set of all possible actions for player $i$. A strategy profile is an assignment of a probability to each player-action pair, with the set of all strategy profiles denoted by $S$.\
For a given strategy profile $s \\in S$, let $u_i(s)$ signify the utility of player $i$ under the strategy profile $s$. Moreover, should player $i$ unilaterally deviate to strategy $a_i$ while all other players stick to their strategies defined by $s$, we denote their utility by $u_i\\left(a_i, s_\{-i\}\\right)$.\
If we define the unilateral gain of player $i$ from deviating to action $a_i$ under strategy profile $s$ as $$ \\varphi_\{i, a_i\}(s) = \\max \\left\\\{0, u_i\\left(a_i, s_\{-i\}\\right) - u_i(s)\\right\\\}, $$ this represents the increased utility from possible deviation, being exactly $0$ when deviation is not beneficial.\
Now, we define a function $f : S \\rightarrow S$, which is a mapping from each strategy profile to a new one, noted as $s^\{\\prime\}$. This mapping is given by:\
\\begin\{equation\}\\label\{eq5\}\
    \\begin\{aligned\}\
s_i^\{\\prime\}\\left(a_i\\right) & =\\frac\{s_i\\left(a_i\\right)+\\varphi_\{i, a_i\}(s)\}\{\\sum_\{b_i \\in A_i\} s_i\\left(b_i\\right)+\\varphi_\{i, b_i\}(s)\} \\\\\
& =\\frac\{s_i\\left(a_i\\right)+\\varphi_\{i, a_i\}(s)\}\{1+\\sum_\{b_i \\in A_i\} \\varphi_\{i, b_i\}(s)\} .\
\\end\{aligned\}\
\\end\{equation\}\
where the numerator represents the sum of the original probability $s_i(a_i)$ and the deviation gain $\\varphi_\{i, a_i\}(s)$ for action $a_i$. With the denominator serving as a normalization factor, we ensure that the updated strategy $s'_i$ of player $i$ remains a probability distribution over $A_i$, since the probabilities still sum up to $1$.\
It is crucial that we show this function $f$ is well-defined, meaning it generates a unique outcome for each strategy profile $s$, and that this outcome always belongs to the set of all strategy profiles $S$.\
\
\
This function $f$ can be interpreted as a mapping from an existing strategy profile $s$ to a new strategy profile $s^\{\\prime\}$, where each player $i$ adjusts their choice of action based on the precedent strategy profile. Specifically, for any action $a_i \\in A_i$ that yields a higher utility (and therefore, can be considered as a 'better response' to the original strategy $s$), the probability of choosing $a_i$, denoted by $s_i^\{\\prime\}\\left(a_i\\right)$, is increased in the new strategy profile $s^\{\\prime\}$. This shift in probability mass towards 'better response' actions is reflective of players' rational tendencies to optimize their utilities.\
\
The function $f$, as previously defined, is continuous by construction. This continuity hinges on that of each component function $\\varphi_\{i, a_i\}$, each of which depends continuously on the strategy profiles.\
The strategy space $S$ is assumed to be both convex and compact. The latter property is ensured by the fact that each $s_i(a_i)$ represents a probability and thus is bound between $0$ and $1$ for all $i$ and $a_i$. The convexity of $S$ follows from the definition of mixed strategies, which are essentially convex combinations of pure strategies.\
Given these properties, and since $f$ is a self-map on $S$ (i.e., $f: S \\rightarrow S$), we can apply Corollary 22 (assuming it provides a general fixed-point theorem for continuous functions on convex compact sets), which then guarantees that function $f$ has at least one fixed point.\
We now wish to ascertain that the fixed points of $f$ coincide with Nash equilibria.\
If $s$ is a Nash equilibrium, by definition no player $i$ can unilaterally deviate to improve their utility, which implies that all $\\varphi_\{i, a_i\}$'s are zero. Hence, the updated strategy profile under function $f$ remains unchanged, that is, $s^\{\\prime\} = f(s) = s$, confirming that every Nash equilibrium is a fixed point of $f$.\
\
Consider a fixed point $s$ of our defined function $f$. Due to the linearity of expectation, there must exist at least one action $a_i^\{\\prime\}$ in the support of player $i$'s mixed strategy $s_i$ such that its expected utility $u_\{i, a_i^\{\\prime\}\}(s)$ is less than or equal to the expected utility of the mixed strategy, $u_i(s)$.\
By the definition of the $\\varphi$ function, we have $\\varphi_\{i, a_i^\{\\prime\}\}(s)=0$. Since $s$ is a fixed point of the function $f$, the updated strategy under this action, $s_i^\{\\prime\}\\left(a_i^\{\\prime\}\\right)$, equals the probability in the original strategy, $s_i\\left(a_i^\{\\prime\}\\right)$.\
Referring back to the expression defining $s_i^\{\\prime\}\\left(a_i^\{\\prime\}\\right)$  Equation \\ref\{eq5\}, we observe that the numerator, $s_i\\left(a_i^\{\\prime\}\\right) + \\varphi_\{i, a_i^\{\\prime\}\}(s)$, simplifies to $s_i\\left(a_i^\{\\prime\}\\right)$ (since $\\varphi_\{i, a_i^\{\\prime\}\}(s) = 0$) and is positive due to $a_i^\{\\prime\}$ being in the support of $s_i$. This necessitates the denominator, $1 + \\sum_\{b_i \\in A_i\} \\varphi_\{i, b_i\}(s)$, to be $1$ in order to maintain equality. As such, for any player $i$ and any strategy $b_i \\in A_i, \\varphi_\{i, b_i\}(s)$ must indeed be $0$.\
Recalling the definition of $\\varphi$, this is possible only when no player can enhance their expected payoff by unilaterally deviating to a pure strategy, which by definition means that the strategy profile $s$ is a Nash equilibrium. Thus, we have shown that every fixed point of $f$ is indeed a Nash equilibrium.\
\\end\{proof\}\
\
\\subsubsection\{Nash's equilibrium theorem in differential game\}\
In light of the significant implications of Nash's Theorem for future advancement, particularly in the realm of differential games, a succinct introduction is provided herein. However, an extensive exploration of this topic is beyond the scope of this discussion.\
Nash equilibrium in differential games has been the subject of significant research. Differential games involve dynamic decision-making and are characterized by the players' strategies being functions of time. Here are some references that discuss Nash equilibria in differential games:\
"Nash equilibria in nonzero-sum differential games with impulse control" - This paper studies a class of two-player nonzero-sum differential games where one player uses ordinary controls while the other uses impulse controls \\citep*\{sadana2021nash\}.\
"Feedback Nash Equilibria in Differential Games with Impulse Control" - This work explores a class of deterministic finite-horizon two-player nonzero-sum differential games where players are endowed with different strategies and feedback Nash equilibria are analyzed \\citep*\{sadana2022feedback\}.\
"Characterization and Computation of Local Nash Equilibria in Continuous Games" - This paper provides a framework for the characterization and computation of differential Nash equilibria in continuous games. It discusses mathematical preliminaries, game formulation, and proposes a steepest descent algorithm for numerical computation of differential Nash equilibria \\citep*\{ratliff2013characterization\}.\
"Nash Equilibria and Bargaining Solutions of Differential Bilinear Games" - This paper investigates Nash equilibria and Nash bargaining problems governed by bilinear differential games. It discusses the theoretical and numerical aspects of Nash equilibria in differential bilinear games \\citep*\{cala2021nash\}.\
These references demonstrate the diverse applications and analyses of Nash equilibria in differential games, covering topics such as impulse control, feedback strategies, and computation methods. The research in this area contributes to a deeper understanding of strategic interactions in dynamic decision-making environments.\
\
\\section\{Application of Nash's equilibria theorem\}\
\
\\subsection\{Cournot oligopoly\}\
The Cournot oligopoly, named after French mathematician Augustin Cournot, is an economic model that describes a market structure where firms compete on the quantity of output they produce independently and simultaneously. This model is applicable when companies produce identical or standardized goods, cannot collude or form a cartel, have the same view of market demand, and are familiar with competitor operating costs.\
\
In a Cournot Oligopoly model, firms simultaneously choose quantities for maximizing their respective profits. This strategic decision making can be characterized by Nash equilibrium. Augustin Cournot first developed this model in 1838, but Nash further expanded upon it.\
Nash's theorem, which includes the concept of Nash Equilibrium, gives a solution concept of a non-cooperative game involving two or more players in which each player is assumed to know the equilibrium strategies of the other players, and no player has anything to gain by changing only their own strategy.the motivation for Cournot oligopoly as an application of Nash's theorem is to showcase how firms in an oligopoly interact with each other strategically, how they can achieve their respective maximum profits, and how a stable state (Nash Equilibrium) can be achieved where no firm seeks to change its output unilaterally. It provides a mathematical and strategic foundation for understanding how real-world oligopolies operate.\
\
In the Cournot model, firms determine the quantity they want to sell to the market, and prices are subsequently determined based on a presumed process of market clearing. The basic Cournot model of quantity competition leads to the intuitive result that equilibrium prices are higher when markets are more concentrated. The market-clearing price is inversely related to the total supply of a product in a particular market (higher total supply leads to a lower market prices, and lower total supply leads to higher market prices) \\cite\{cournot1927researches\}.\
\
The Cournot model also shows that under relatively mild assumptions, the theoretical outcome under Cournot competition between firms mimics the theoretical outcome of a two-stage model in which firms first choose their capacity level of production and subsequently compete on price.\
\
Cournot's duopoly model, a specific case of the Cournot oligopoly, was the first to introduce the concept of game theory. In this model, two firms operate in a limited market, and both companies will receive profits derived from a simultaneous decision made by both on how much to produce, and also based on their cost functions.\
\
The Cournot model implies that output is greater in a Cournot duopoly than in a monopoly but still lower than perfect competition. Prices are also lower in a Cournot duopoly, but higher than perfect competition. Cournot equilibria are also a subset of Nash equilibria, and so the equilibrium derived is one from which neither player will likely deviate.\
\
\
It's important to note that while the Cournot model provides valuable insights into the behavior of firms in an oligopoly, it has its limitations. For instance, it assumes that firms cannot collude, which is not always the case in real-world markets. Furthermore, it assumes that firms compete on quantity, not price, which may not hold true in all market situations.\
\
\
Take an economy involving $N$ firms, where each firm $i$ decides upon its production quantity denoted $q_i$. This firm also operates along a cost function $c_i(q)$, indicating that producing $q$ units will cost $c_i(q)$.\
The total quantity produced by all firms in the economy, denoted $Q$, can be calculated as the sum of each firm's production, i.e.,\
\\begin\{equation\}\
Q = q_1 + \\ldots + q_N\
\\end\{equation\}\
Each product's unit price is determined by an inverse demand function, denoted $p(Q)$.\
We can then calculate the profit for each firm $i$, denoted $\\pi_i\\left(q_1, \\ldots, q_N\\right)$, using the formula:\
\\begin\{equation\}\
\\pi_i\\left(q_1, \\ldots, q_N\\right) = p(Q) q_i - c_i\\left(q_i\\right)\
\\end\{equation\}\
This last equation states that the profit for each firm is the difference between the total revenue ($p(Q)q_i$) and the cost of production ($c_i(q_i)$).\
\
At a Nash equilibrium, denoted as $\\left(q_1^*, \\ldots, q_N^*\\right)$, firm $i$ selects output $q$ to maximize its profit, taking the outputs of all other firms as given, i.e., assuming that for $j \\neq i$, firm $j$ produces $q_j^*$. Under the plausible assumption that each firm seeks to maximize its profits, the Nash equilibrium should be a solution to:\
\\begin\{equation\}\
\\frac\{\\partial \\pi_1\}\{\\partial q_1\}\\left(q_1^*, \\ldots, q_N^*\\right) = \\ldots = \\frac\{\\partial \\pi_N\}\{\\partial q_N\}\\left(q_1^*, \\ldots, q_N^*\\right) = 0\
\\end\{equation\}\
This corresponds to the condition that, at equilibrium, each firm maximizes its profit given the outputs of the other firms, which in turn implies that no firm has incentives to deviate unilaterally from its output level.\
\
\\subsubsection\{Identical firms\}\
Let us now explore a specific case of this problem where all firms are identical in terms of their cost structure and face the same inverse demand function. We consider the following assumptions:\
The inverse demand function is given by: $p(Q) = \\alpha - bQ$. Here, $\\alpha$ and $b$ are constants, and $Q$ represents the total quantity produced in the market.\
Each firm has an identical cost of production, such that the production cost function for the $i$-th firm, $c_i\\left(q_\{i\}\\right)$, is given by: $c_i\\left(q_\{i\}\\right) = c q_\{i\}$. Here, $c$ is a constant representing the cost of producing a unit quantity, and $q_\{i\}$ is the quantity produced by the $i$-th firm.\
Proceeding with these assumptions, we can derive the Nash Equilibrium for this economy by substituting these functions into the first-order conditions previously established to ascertain the optimal production quantity for each firm.\
\\subsubsection\{Monopoly\}\
From our given conditions, we first consider the case of a monopoly. That is, a situation where there is only one firm in the market. This firm\'92s profit function can be expressed as:\
\\begin\{equation\}\
\\pi(q) = (\\alpha - bq)q - cq .\
\\end\{equation\}\
To maximize its profits, the monopoly firm adjusts its production output $q$ such that it satisfies the first order condition, i.e., the derivative of the profit function equates to zero; $\\pi'(q) = 0$. This results in:\
\\begin\{equation\}\
0 = \\pi'(q) = \\alpha - 2bq - c\
\\end\{equation\}\
Solving this equation for $q$, we find the optimal production quantity $q^* = \\frac\{\\alpha - c\}\{2b\}$. Given this optimal $q$, we derive the following market outcomes:\
\\begin\{enumerate\}\
    \\item The total production $Q$ in the market is then $Q = q^* = \\frac\{\\alpha - c\}\{2b\}$.\
    \\item The price of the good, according to the inverse demand function, turns out to be:\
\\begin\{equation\}\
p(q^*) = \\alpha - b\\left(\\frac\{\\alpha - c\}\{2b\}\\right) = \\frac\{\\alpha + c\}\{2\}\
\\end\{equation\}\
\\item \
Finally, the resulting profit of the monopoly firm is:\
\\begin\{equation\}\
\\pi(q^*) = \\frac\{\\alpha + c\}\{2\} \\cdot \\frac\{\\alpha - c\}\{2b\} - c \\cdot \\frac\{\\alpha - c\}\{2b\} = \\frac\{(\\alpha - c)^2\}\{4b\}\
\\end\{equation\}\
\\end\{enumerate\}\
\\subsubsection\{Cournor Duopoly\}\
Next, we explore the case when $N=2$, i.e., there are two identical firms in the market, creating a Cournot duopoly. Each firm, denoted by $i$, has a profit function given by:\
\\begin\{equation\}\
\\pi_i (q_1, q_2) = (\\alpha - b(q_1 + q_2))q_i - c q_i\
\\end\{equation\}\
In this scenario, the partial derivatives of the profit function for each firm with respect to their own output are:\
\\begin\{equation\}\
\\frac\{\\partial \\pi_1\}\{\\partial q_1\} (q_1, q_2) = \\alpha - 2 b q_1 - b q_2 - c\
\\end\{equation\}\
and\
\\begin\{equation\}\
\\frac\{\\partial \\pi_2\}\{\\partial q_2\} (q_1, q_2) = \\alpha - b q_1 - 2 b q_2 - c\
\\end\{equation\}\
These expressions represent the incremental change in each firm's profit for a small change in its own output while holding the output of the other firm constant. In the next step, we proceed by setting these to zero to find the Nash equilibrium of this duopolistic market.\
\
To derive firm 1's optimal output response to firm 2's output, we set the first-order condition $\\partial \\pi_1 / \\partial q_1 = 0$ and solve for $q_1$. This gives us firm 1's best response function:\
\\begin\{equation\}\
BR_1\\left(q_2\\right) = \\frac\{\\alpha+c - bq_2\}\{2b\}\
\\end\{equation\}\
Similarly, by using the same approach for firm 2, we obtain firm 2's best response function as:\
\\begin\{equation\}\
BR_2\\left(q_1\\right) = \\frac\{\\alpha+c - bq_1\}\{2b\}\
\\end\{equation\}\
The Nash equilibrium $(q_1^*, q_2^*)$ for this Cournot duopoly game is then determined by simultaneously solving the following system of linear equations:\
\\begin\{align\}\
2bq_1^* + bq_2^* &= \\alpha - c \\\\\
bq_1^* + 2bq_2^* &= \\alpha - c\
\\end\{align\}\
Solving these equations, we find that:\
\\begin\{equation\}\
q_1^* = q_2^* = \\frac\{\\alpha - c\}\{3b\}\
\\end\{equation\}\
This indicates that in a Cournot duopoly with linear demand and identical costs, both firms produce the same quantity in equilibrium.\
\
\\begin\{enumerate\}\
    \\item Let us consider the total production $Q$. Given a model where $\\alpha$, $b$, and $c$ are parameters, the equation for total production can be written as: $Q = \\frac\{2(\\alpha-c)\}\{3b\}$.\
    \\item \
Under the same conditions, the price $p$, depending on $q_1^*$ and $q_2^*$, can be calculated from the formula: $$p\\left(q_1^*, q_2^*\\right)=\\alpha-b\\left(\\frac\{2(\\alpha-c)\}\{3 b\}\\right)=\\frac\{\\alpha+2 c\}\{3\}$$\
\\item \
Finally, the firm's profit $\\pi$, also depending on $q_1^*$ and $q_2^*$, is obtained as follows: $$ \\pi\\left(q_1^*, q_2^*\\right)=\\frac\{\\alpha+2 c\}\{3\}\\left(\\frac\{2(\\alpha-c)\}\{3 b\}\\right)-c\\left(\\frac\{2(\\alpha-c)\}\{3 b\}\\right)=\\frac\{2(\\alpha-c)^2\}\{9 b\}.$$ This equation provides insight into the firm's profitability under the given economic conditions.\
\\end\{enumerate\}\
\
In equilibrium, we find that consumers are strictly better off compared to a scenario of monopoly control. This is due to the fact that under competitive conditions, the total production is markedly higher, leading to lower prices. However, this reduced price level in equilibrium, while beneficial to consumers, results in decreased profits for the firms. Hence, firms are worse off under this market situation compared to a monopoly. Rigorous proofs of these claims require detailed analysis of the supply and demand dynamics.\
\\subsubsection\{Cartel\}\
A cartel formation could be another alternative for the two firms. In this case, they would collectively produce a quantity $Q$, where each firm's contribution, i.e., $q_1$ and $q_2$, is exactly half of the total, $q_1=q_2=\\frac\{Q\}\{2\}$. The firms strategically select $Q$ to maximize their joint profits. In this scenario, they would adopt the optimal solution from a monopoly setting, $Q=\\frac\{\\alpha-c\}\{2b\}$. The profits are thus distributed equally, resulting in each firm taking a share of $\\frac\{(\\alpha-c)^2\}\{8b\}$.\
Although this profit is higher than what each firm would make under a duopoly, it should be noted that the firms aren't invoking best-response strategies in this setup. Consequently, the firms could potentially increase their individual profits by breaking this agreement and pursuing their profit-maximizing strategies. The stability of the cartel hence depends on the ability of the firms to maintain this agreement.\
\
\\subsubsection\{$N=3$ firms\}\
Consider a scenario with $N=3$ firms. The profit function $\\pi_i$ for each firm $i$, where $i=1,2,3,$ is given by\
$$\
\\pi_i\\left(q_1, q_2, q_3\\right)=\\left(\\alpha-b\\left(q_1+q_2+q_3\\right)\\right) q_i-c q_i.\
$$\
We derive the best-response functions by taking the first order conditions w.r.t. $q_i$. This leads to the following system of equations:\
$$\
\\begin\{aligned\}\
\\frac\{\\partial \\pi_1\}\{\\partial q_1\}\\left(q_1, q_2, q_3\\right)=\\alpha-2bq_1-bq_2-bq_3-c=0,\\\\\
\\frac\{\\partial \\pi_2\}\{\\partial q_2\}\\left(q_1, q_2, q_3\\right)=\\alpha-bq_1-2bq_2-bq_3-c=0,\\\\\
\\frac\{\\partial \\pi_3\}\{\\partial q_3\}\\left(q_1, q_2, q_3\\right)=\\alpha-bq_1-bq_2-2bq_3-c=0.\
\\end\{aligned\}\
$$\
We solve this system of linear equations and find a symmetric Nash Equilibrium where each firm produces the quantity $q_1^*=q_2^*=q_3^*=\\frac\{\\alpha-c\}\{4b\}$.\
\
\\begin\{enumerate\}\
    \\item Firstly, let's evaluate the total production $Q$ under a scenario where we have three firms. Applying the defined values from the Nash Equilibrium solution of $q_1^*=q_2^*=q_3^*=\\frac\{\\alpha-c\}\{4b\}$, we achieve a total product of $Q = 3 \\times \\frac\{\\alpha-c\}\{4b\} = \\frac\{3(\\alpha-c)\}\{4 b\}$.\
    \\item \
Secondly, let's consider the determination of price $p$, again leveraging the Nash Equilibrium solutions for $q_1^*$, $q_2^*$, and $q_3^*$. The estimated market price, according to our model, happens to be $p\\left(q_1^*, q_2^*, q_3^*\\right)=\\frac\{\\alpha+3 c\}\{4\}$.\
\\item \
Lastly, the calculation of a firm's profit $\\pi$. Bearing in mind our Nash Equilibrium solutions for $q_1^*$, $q_2^*$, and $q_3^*$, and assuming no discrepancy in the profits earned by each firm, we find $\\pi\\left(q_1^*, q_2^*, q_3^*\\right)=\\frac\{3(\\alpha-c)^2\}\{16 b\}$.\
\\end\{enumerate\}\
\\subsubsection\{General case\}\
Now, let's consider the general case with $N$ firms. For this situation, we need to solve the following system of linear equations:\
$$\
\\begin\{aligned\}\
2 b q_1^* + \\sum_\{i=2\}^\{N\} b q_i^* & =\\alpha-c \\\\\
b q_1^* + 2 b q_2^* + \\sum_\{i=3\}^\{N\} b q_i^* & =\\alpha-c \\\\\
& \\vdots \\\\\
\\sum_\{i=1\}^\{N-1\} b q_i^* + 2 b q_N^*& =\\alpha-c\
\\end\{aligned\}\
$$\
By solving this system, we obtain $q_1^*=\\ldots=q_N^*=\\frac\{\\alpha-c\}\{(N+1) b\}$.\
\
\\begin\{enumerate\}\
    \\item The total production $Q$ is obtained by summing the equilibrium quantities produced by all firms, and can be expressed as $Q = \\frac\{N(\\alpha-c)\}\{(N-1) b\}$.\
    \\item \
The price $p$, dependent on $q_1^*, \\ldots, q_N^*$, is calculated by $p\\left(q_1^*, \\ldots, q_N^*\\right)=\\frac\{\\alpha+N c\}\{N+1\}$.\
\\item \
The profit $\\pi$ of each firm, under this equilibrium, can be calculated by $\\pi\\left(q_1^*, \\ldots, q_N^*\\right)=\\frac\{N(\\alpha-c)^2\}\{(N+1)^2 b\}$.\
\\end\{enumerate\}\
\
Clearly, these results are generalizations of what we observed in the specific cases of two and three firms.\
\
\
\\subsubsection\{Perfect Competition\}\
As the number of firms $N$ tends to infinity, we can analyze the limit behaviors of the total production, price, and profit.\
\
\\begin\{enumerate\}\
    \\item Analyzing total production: We examine the limit as $N \\rightarrow +\\infty$ of total production $\\frac\{N(\\alpha-c)\}\{(N-1) b\}$, which can be shown to be $$\
\\lim _\{N \\rightarrow \\infty\} \\frac\{N(\\alpha-c)\}\{(N-1) b\}=\\frac\{\\alpha-c\}\{b\}.\
$$ This indicates that as the number of firms increase indefinitely, the total production approaches $\\frac\{\\alpha-c\}\{b\}$.\
\
\\item Analyzing price: When we examine the limit as $N \\rightarrow +\\infty$ of price $\\frac\{\\alpha+N c\}\{N+1\}$, we find $$\
\\lim _\{N \\rightarrow \\infty\} \\frac\{\\alpha+N c\}\{N+1\}=c.\
$$ This suggests that as the market becomes highly competitive (as $N$ goes to infinity), price converges to the marginal cost $c$.\
\
\\item Analyzing profit: Examining the limit as $N \\rightarrow +\\infty$ of profit $\\frac\{N(\\alpha-c)^2\}\{(N+1)^2 b\}$, we find $$\
\\lim _\{N \\rightarrow \\infty\} \\frac\{N(\\alpha-c)^2\}\{(N+1)^2 b\}=0.\
$$ This suggests that the profit per firm tends to zero as the number of firms increases indefinitely.\
\
\\end\{enumerate\}\
\
\
These results cohere with classical economic theories of perfect competition where, in equilibrium, price equals marginal cost (which in our model is $c$) and profit becomes zero.\
\
\\subsection\{Tullock Contest\}\
The Tullock contest, named after economist Gordon Tullock, is a concept in the field of contest theory and rent-seeking \\citep*\{tullock1967welfare,harsanyi1977social\}. It refers to a model where individuals or groups compete by expending resources in order to win a prize. The resources expended by the participants determine their probability of winning the prize. The Tullock contest has been widely studied and has various applications in economic and social interactions.\
The historical development of the Tullock contest is connected to the concept of rent-seeking, which was introduced by Tullock in 1967. Rent-seeking refers to the expenditure of resources in order to gain an economic rent, such as a government subsidy or a monopoly privilege. The Tullock contest provides a framework for analyzing such interactions where individuals or groups engage in costly efforts to secure a prize or rent.\
The Tullock contest has been the subject of extensive research and has been applied in various contexts, including models of imperfectly discriminating contests and the analysis of information effects on equilibrium efforts and payoffs. It has also been generalized to encompass different types of contests, allowing for the analysis of new contest structures and interactions. \
\
In the basic lottery model, $n$ players are assumed to pay a bid denoted by $x_i$ upfront, competing to win a prize with a value $v \\in \\mathbb\{R\}_\{+\}$. The probability that player $i$ wins is defined as their contribution to the total bid, given by: $$\
\\frac\{x_i\}\{\\sum_\{j=1\}^n x_j\}.\
$$\
Now, consider a Tullock contest that introduces another level of complexity. We introduce a parameter $r>0$, and the bid $x_i$ from each player is scaled by a power of $r$. This transformation effects the probability of player $i$ winning as: $$\
\\frac\{x_i^r\}\{\\sum_\{j=1\}^n x_j^r\}.\
$$\
Accordingly, the expected profit of player $i$ in a Tullock contest becomes: $$\
\\frac\{x_i^r\}\{\\sum_\{j=1\}^n x_j^r\} v-x_i.\
$$\
The expected profit reflects the scaled likelihood of winning the contest, multiplied by the value of the prize, and finally subtracting the cost of the player\'92s own bid.\
\
In a Tullock contest, we wish to derive the symmetric Nash equilibrium strategy for this game. Under Nash equilibrium, each player seeks to maximize its expected profit. The first-order condition (FOC), obtained by differentiating the expected profit and setting equal to zero yields:\
$$\\frac\{r x_i^\{r-1\} \\cdot \\sum_\{j=1\}^n x_j^r-r x_i^\{2 r-1\}\}\{\\left(\\sum_\{j=1\}^n x_j^r\\right)^2\} v-1=0.$$\
This FOC equates the change in the expected profit from a marginal change in the bid to zero. In a symmetric equilibrium, it is presumed that each player makes the same bid $x_i$. Substituting this into the first order conditions and simplifying, we specify the symmetric equilibrium bid in terms of the parameters of the game and the number of players:\
$$\\frac\{r n x_i^\{2 r-1\}-r n x_i^\{2 r-1\}\}\{n^2 x_i^\{2 r\}\} v=1 \\Longrightarrow \\frac\{r(n-1)\}\{n^2\} v=x_i.$$\
Thus, in the symmetric Nash equilibrium of a Tullock contest, each player bids an amount of $\\frac\{r(n-1)\}\{n^2\} v$.\
\
Let's analyze the symmetric Nash Equilibrium of the Tullock contest and whether it is a generalized Evolutionarily Stable Strategy (ESS) for different values of $r$.\
When $r \\leq \\frac\{n\}\{n-1\}$, we find the value of the symmetric Nash equilibrium strategy to be $x_i^*=\\frac\{r(n-1)\}\{n^2\} v$. However, for values of $r>\\frac\{n\}\{n-1\}$, utilizing the strategy $x^*=\\frac\{r(n-1)\}\{n^2\} v$ results in an overpayment by the player.\
Interestingly, when $r \\leq \\frac\{n\}\{n-1\}$, the unique pure strategies symmetric Nash Equilibrium is not a generalized ESS. Intuitively, a small increase in its bid from $x_i^*$ provides a slight increase in $i$ 's expected payoff, due to a small shift in the probability of winning in $i$'s favor. Conversely, the expected payoffs for other players decrease slightly.\
To demonstrate this formally, consider the optimization problem: maximize $\\Delta u_1 = u_1\\left(y, x^*, \\ldots, x^*\\right)-u_i\\left(y, x^*, \\ldots, x^*\\right)$ subject to $y \\geq 0$, where $x^*$ is the symmetric Nash equilibrium of the Tullock contest. The expression to be maximized represents the difference in $i$ 's utility when they deviate from the Nash equilibrium to $y$.\
The first order necessary condition for a maximum $(\\left.\{ *\}\\right)$: \
$$\\frac\{\\partial u_1\\left(y, x^*, \\ldots, x^*\\right)\}\{\\partial y\}=0,$$ applies at $y=x^*$, because $x^*$ is a Nash equilibrium.\
This mathematical analysis provides the nuanced understanding of how player strategies evolve under different parameter conditions.\
\
Consider the derivative of the utility function $u_i(y, x^*, ..., x^*)$ with respect to $y$:\
$$\
\\frac\{\\partial u_i\\left(y, x^*, \\ldots, x^*\\right)\}\{\\partial y\}=-\\frac\{\\left(x_i^*\\right)^r \\cdot r y^\{r-1\}\}\{\\left(y^r+(n-1)\\left(x_i^*\\right)^r\\right)^2\} v.\
$$\
We can evaluate this at the Nash equilibrium strategy $x^*$ by plugging in $y=x^*$: \
\\begin\{equation\}\
    \\begin\{aligned\}\
\\frac\{\\partial u_i\\left(x^*, x^*, \\ldots, x^*\\right)\}\{\\partial y\}=&-\\frac\{r\\left(x_i^*\\right)^\{2 r-1\}\}\{\\left((x^*)^r+(n-1)(x^*)^r\\right)^2\}v\\\\\
=&-\\frac\{r\\left(x_i^*\\right)^\{2 r-1\}\}\{\\left(n(x_i^*)^r\\right)^2\} v.\
\\end\{aligned\}\
\\end\{equation\}\
As $(x_i^*)^r > 0$ and $r>0$, the denominator is positive, and thus this derivative is negative.\
Thus, when $y=x^*$, the derivative of player $1$'s expected payoff with respect to $y$ is greater than that of player $i$: $$\
\\frac\{\\partial\\left(u_1\\left(y, x^*, \\ldots, x^*\\right)-u_i\\left(y, x^*, \\ldots, x^*\\right)\\right)\}\{\\partial y\}>0.\
$$ This indicates that deviating from $x^*$ can increase player $1$'s expected payoff.\
\
Given that $x^*$ is an interior point, we note that it does not maximize relative payoffs. As a result, $x^*$ is not a generalized Evolutionarily Stable Strategy (ESS).\
To find an ESS, we consider the first order condition from equation (12), which is given as:\
$$\
\\frac\{\\partial\\left(u_1(y, x, \\ldots, x)-u_i v\\right)\}\{\\partial y\}=\\frac\{n(y x)^r\}\{\\left(y^r+(n-1) x^r\\right)^2 y\} v-1=0.\
$$\
Under a symmetric ESS, we should have $y=x$. Substituting $y$ with $x$ in the first order condition and simplifying, we find:\
$$\
\\begin\{aligned\}\
n x^\{2 r\} r v = n^2 x^\{2 r+1\} v \\Longrightarrow x=\\frac\{r v\}\{n\}.\
\\end\{aligned\}\
$$\
Therefore, when $r \\leq \\frac\{n\}\{n-1\}$, the symmetric ESS of this game is represented by $x^*=\\frac\{r v\}\{n\}$.\
\
\
\
\
\
\
\
\
\
\
\
\
\
\
\
\
\
\
\
\
\
\
\
\
\
\
\
\
\
\
\
\
\
\
\
\
\
\
\
\
\
\
\
\
\
\
\
\
\
\
\
\
\
\
\
\
\
\
\
\
\
\
\
\
\
\
\
\
\
\
\
\
\
\
\\newpage\
\\bibliographystyle\{plainnat\}\
\\bibliography\{game.bib\}\
\
\\end\{document\}\
}